# The # character at the beginning of a line indicates a comment. Use
# comments to describe your configuration.

input {
    beats {
        port => "{{ logstash_port }}"
    }
}

filter {

    # Dstat header message pattern
    if ![message_kind] or [message_kind] == "dstat" {
        grok {
            patterns_dir => ["/etc/logstash/patterns"]
            patterns_files_glob => "env-logs-patterns"
            match => { "message" => "(?<message>(?m)^\"Dstat \d+(\.\d+)* CSV output\".*)\"Date:\",\"%{TIMESTAMP:ts} \w+\"\n\n%{DATA:[csv][header][1]}\n%{DATA:[csv][header][2]}$" }
            overwrite => [ "message" ]
            add_field => { "message_type" => "dstat_header" }
            remove_tag => [ "message_failure" ]
            tag_on_failure => [ "message_failure" ]
        }
        if [message_type] == "dstat_header" {
            if ![message_kind] {
                mutate {
                    add_field => { "message_kind" => "dstat" }
                }
            }
            ruby {
                init => "@@source_header = {}"
                code => "h1 = event.get('[csv][header][1]').gsub(' ','_').split(','); h2 = event.get('[csv][header][2]').gsub(' ','_').split(','); h2.each_index{ |i| if h1[i] == nil || h1[i] == '' then h1[i] = h1[i-1] end }; h2.each_index{ |i| h = 'dstat.' + h1[i][1..-2] + '.' + h2[i][1..-2]; h1[i] = h }; @@source_header[event.get('source')] = h1"
                remove_field => [ "[csv][header][1]", "[csv][header][2]" ]
            }
        }
    }
    
    # GC log header message pattern
    if ![message_kind] or [message_kind] == "gc" {
        grok {
            patterns_dir => ["/etc/logstash/patterns"]
            patterns_files_glob => "env-logs-patterns"
            match => { "message" => "^Java.*\nMemory:\s*%{DATA:[gc][mem][page_size]} page, physical %{DATA:[gc][mem][phys][total]}\(%{DATA:[gc][mem][phys][free]} free\), swap %{DATA:[gc][mem][swap][total]}\(%{DATA:[gc][mem][swap][free]} free\)\nCommandLine flags:\s+%{DATA:[gc][flags]}\s*(\n|$)" }
            overwrite => [ "message" ]
            add_field => { "message_type" => "gc_header" }
            remove_tag => [ "message_failure" ]
            tag_on_failure => [ "message_failure" ]
        }
        if [message_type] == "gc_header" {
            if ![message_kind] {
                mutate {
                    add_field => { "message_kind" => "gc" }
                }
            }
        }
    }

    # Thread dump header message pattern
    if ![message_kind] or [message_kind] == "td" {
        grok {
            patterns_dir => ["/etc/logstash/patterns"]
            patterns_files_glob => "env-logs-patterns"
            match => { "message" => "^%{TIMESTAMP:ts}\n(?<message>Full thread dump Java.*)" }
            overwrite => [ "message" ]
            add_field => {
                "message_kind" => "td"
                "message_type" => "td_header"
            }
            remove_tag => [ "message_failure" ]
            tag_on_failure => [ "message_failure" ]
        }
        if [message_type] == "td_header" {
            if ![message_kind] {
                mutate {
                    add_field => { "message_kind" => "td" }
                }
            }
            ruby {
                init => "@@source_ts = {}"
                code => "@@source_ts[event.get('source')] = event.get('ts')"
            }
        }
    }

    if [message_kind] {
    
        # Remember source message kind
        ruby {
            init => "@@source_kind = {}"
            code => "@@source_kind[event.get('source')] = event.get('message_kind')"
        }
    } else {

        # Try to restore source message kind from cache
        ruby {
            code => "i = 0; mk = nil; while mk == nil && i < 5 do if i > 0 then sleep(0.01) end; mk = @@source_kind[event.get('source')]; i += 1 end; if mk != nil then event.set('message_kind', mk) end"
        }
    }
    
    # Default log full message pattern (splitting to timestamp, loglevel, thread, emitter and remaining message)
    if ![message_kind] or [message_kind] == "log" {
        grok {
            patterns_dir => ["/etc/logstash/patterns"]
            patterns_files_glob => "env-logs-patterns"
            match => { "message" => "^%{TIMESTAMP:ts}\s*%{LOGLEVEL}\s*\[%{DATA}\]\s*\(%{DATA:[thread][name]}\)\s*%{TIME}\s*\|-%{LOGLEVEL:level}\s+in\s+%{DATA:emitter}\s-\s%{MLGREEDY:message}" }
            match => { "message" => "^%{TIMESTAMP:ts}\s*%{LOGLEVEL}\s*\[%{DATA}\]\s*\(%{DATA}\)\s*IFT:\s*%{DATA:[module][name]}:\s*\[%{DATA:[thread][name]}\]\s*%{TIME}\s+%{LOGLEVEL:level}\s+%{DATA:emitter}:%{NONNEGINT:[line][number]:int}\s-\s%{MLGREEDY:message}" }
            match => { "message" => "^\[?%{TIMESTAMP:ts}\]?\s*\[%{LOGLEVEL:level}\s*\]\s*(\[%{DATA:[thread][name]}\]\s*\[%{DATA:emitter}\]\s*)?%{MLGREEDY:message}" }
            match => { "message" => "^%{TIMESTAMP:ts}\s*\[%{LOGLEVEL:level}\s*\]\s*(\[%{DATA:emitter}\]\s*\[%{DATA:[thread][name]}\]\s*)?-\s*%{MLGREEDY:message}" }
            match => { "message" => "^%{TIMESTAMP:ts}\s*%{LOGLEVEL:level}\s*\[%{DATA:emitter}\]\s*\(%{DATA:[thread][name]}\)\s%{MLGREEDY:message}" }
            match => { "message" => "^%{TIMESTAMP:ts}\s*%{LOGLEVEL:level}\s*%{DATA:emitter}(:\d+)?\s*-\s*%{MLGREEDY:message}" }
            match => { "message" => "^%{TIMESTAMP:ts}\s*\[%{DATA:[thread][name]}\]\s*%{LOGLEVEL:level}\s+%{DATA:emitter}:\s*-\s*%{MLGREEDY:message}" }
            match => { "message" => "^%{TIME:tm}\s*\|-%{LOGLEVEL:level}\s+in\s+%{DATA:emitter}\s-\s%{MLGREEDY:message}" }
            match => { "message" => "^%{TIME:tm}\s*\[%{LOGLEVEL:level}\s*\]\s*\[%{DATA:emitter}\]\s*(\[T:\]\s*)?-\s*%{MLGREEDY:message}" }
            match => { "message" => "^%{TIME:tm}\s+%{LOGLEVEL:level}\s*\[%{DATA:[thread][name]}\]\s*%{DATA:emitter}\s*-\s*%{MLGREEDY:message}" }
            match => { "message" => "^%{DATA:[module][name]}:\[%{DATA:[thread][name]}\]\s*%{TIMESTAMP:ts}\s+%{LOGLEVEL:level}\s+%{DATA:emitter}:%{NONNEGINT:[line][number]:int}\s-\s%{MLGREEDY:message}" }
            match => { "message" => "^\[\[%{DATA:[module][name]}\]-\[%{DATA:[module][service]}\]-%{DATA:[thread][name]}\]\s*%{TIME:tm}\s+%{LOGLEVEL:level}\s+%{DATA:emitter}:%{NONNEGINT:[line][number]:int}\s-\s%{MLGREEDY:message}" }
            match => { "message" => "^\[%{DATA:[thread][name]}\s*\|\s*%{DATA:[module][service]}\]\s*%{TIME:tm}\s+%{LOGLEVEL:level}\s+%{DATA:emitter}:%{NONNEGINT:[line][number]:int}\s-\s%{MLGREEDY:message}" }
            match => { "message" => "^(\[%{DATA:[thread][name]}\s*\]\s*)?%{TIME:tm}\s+%{LOGLEVEL:level}\s+%{DATA:emitter}:%{NONNEGINT:[line][number]:int}\s-\s%{MLGREEDY:message}" }
            match => { "message" => "^<%{TIME:tm}><%{DATA:[thread][name]}><%{DATA:emitter}>\s*%{MLGREEDY:message}" }
            match => { "message" => "^%{TIMESTAMP:ts}\s*%{MLGREEDY:message}" }
            match => { "message" => "^\[%{TIME:tm}\]\s*%{MLGREEDY:message}" }
            named_captures_only => true
            overwrite => [ "message" ]
            remove_tag => [ "message_failure" ]
            tag_on_failure => [ "message_failure" ]
            timeout_millis => 0
        }
        if ![message_kind] and !("message_failure" in [tags]) {
            mutate {
                add_field => { "message_kind" => "log" }
            }
        }
    }

    # Safepoint message pattern
    if ![message_kind] or ([message_kind] == "log" and "message_failure" in [tags]) {
        grok {
            patterns_dir => ["/etc/logstash/patterns"]
            patterns_files_glob => "env-logs-patterns"
            match => { "message" => "^\s+vmop\s*\[threads: total initially_running wait_to_block\]\s*\[time: spin block sync cleanup vmop\] page_trap_count\n%{BASE10NUM:uptime:float}:\s*%{DATA:[safepoint][vmop]}\s*\[\s*%{NONNEGINT:[safepoint][threads][total]:int}\s+%{NONNEGINT:[safepoint][threads][initially_running]:int}\s+%{NONNEGINT:[safepoint][threads][wait_to_block]:int}\s*\]\s*\[\s*%{NONNEGINT:[safepoint][time][spin]:int}\s+%{NONNEGINT:[safepoint][time][block]:int}\s+%{NONNEGINT:[safepoint][time][sync]:int}\s+%{NONNEGINT:[safepoint][time][cleanup]:int}\s+%{NONNEGINT:[safepoint][time][vmop]:int}\s*\]\s*%{NONNEGINT:[safepoint][page_trap_count]:int}" }
            add_field => { "message_type" => "safepoint" }
            remove_tag => [ "message_failure" ]
            tag_on_failure => [ "message_failure" ]
            timeout_millis => 0
        }
        if [message_type] == "safepoint" {
            if ![message_kind] {
                mutate {
                    add_field => { "message_kind" => "log" }
                }
            }
            mutate {
                replace => { "message" => "vmop: %{[safepoint][vmop]}" }
            }
        }
    }
    
    # Dstat message pattern
    if [message_kind] == "dstat" and ![message_type] {
        ruby {
            code => "h = @@source_header[event.get('source')]; r = event.get('message').split(','); h.each_index{ |i| if h[i] == 'dstat.system.time' then event.set('ts', r[i]) else event.set(h[i], r[i].to_f) end }; event.set('message', '')"
            remove_tag => [ "message_failure" ]
        }
    }

    # Thread dump message pattern
    if [message_kind] == "td" and ![message_type] {
        grok {
            patterns_dir => ["/etc/logstash/patterns"]
            patterns_files_glob => "env-logs-patterns"
            match => { "message" => "^\"%{DATA:[thread][name]}\"\s+(#%{NONNEGINT:[thread][num]}\s+)?((?<thread.daemon>daemon)\s+)?(prio=%{NONNEGINT:[thread][prio]}\s+)?(os_prio=%{NONNEGINT:[thread][os_prio]}\s+)?tid=%{BASE16NUM:[thread][tid]}\s+nid=%{BASE16NUM:[thread][nid]}\s*%{DATA:[thread][status]}\s*(\[%{BASE16NUM:[thread][ptr]}\]\n\s*java.lang.Thread.State:\s*%{DATA:[thread][state]})?(\n|$)" }
            remove_tag => [ "message_failure" ]
            tag_on_failure => [ "message_failure" ]
        }
        ruby {
            code => "ts = @@source_ts[event.get('source')]; if ts != nil then event.set('ts',  ts) else event.tag('ts_nil') end"
        }
    }

    # GC log full message pattern
    if [message_kind] == "gc" and ![message_type] {
        grok {
            patterns_dir => ["/etc/logstash/patterns"]
            patterns_files_glob => "env-logs-patterns"
            match => { "message" => "^(%{TIMESTAMP:ts}:\s*)?%{BASE10NUM:uptime:float}:\s*%{MLGREEDY:message}" }
            overwrite => [ "message" ]
            remove_tag => [ "message_failure" ]
            tag_on_failure => [ "message_failure" ]
            timeout_millis => 0
        }
    }

    # Default message kind
    if ![message_kind] {
        mutate {
            add_field => { "message_kind" => "log" }
        }
    }

    # Last date processing
    if [ts] {
    
        # Remember last date
        grok {
            patterns_dir => ["/etc/logstash/patterns"]
            patterns_files_glob => "env-logs-patterns"
            match => { "ts" => "^(?<dt>\d{2,4}[-/\.]\d{2}[-/\.]\d{2,4})" }
            tag_on_failure => [ "dt_failure" ]
        }
        if !("dt_failure" in [tags]) {
            ruby {
                init => "@@source_date = {}"
                code => "@@source_date[event.get('source')] = event.get('dt')"
                remove_field => [ "dt" ]
            }
        }
    }
    else if [tm] {
    
        # Constructing timestamp from time and last date 
        ruby {
            code => "i = 0; dt = nil; while dt == nil && i < 10 do if i > 0 then sleep(0.01) end; dt = @@source_date[event.get('source')]; i += 1 end; if dt != nil then event.set('ts',  dt + ' ' + event.get('tm')) else event.tag('dt_nil') end"
            remove_field => [ "tm" ]
        }
    }
    
    # Uptime processing
    if [ts] {

        # Parsing log timestamp
        date {
            locale => "en"
            match => ["ts", "YYYY-MM-dd'T'HH:mm:ss.SSSZ", "YYYY-MM-dd HH:mm:ss.SSS", "YYYY-MM-dd HH:mm:ss,SSS", "YYYY.MM.dd HH:mm:ss.SSS", "YYYY-MM-dd HH:mm:ss", "YYYY.MM.dd HH:mm:ss", "dd.MM.YYYY HH:mm:ss", "dd MMM YYYY HH:mm:ss", "dd-MM HH:mm:ss", "HH:mm:ss" ]
            timezone => "Europe/Moscow"
            target => "@timestamp"
            remove_field => [ "ts" ]
            tag_on_failure => [ "ts_failure" ]
        }
    } else if [uptime] {
    
        # Calculating log timestamp from source start timestamp
        ruby {
            init => "@@source_start = {}"
            code => "ss = @@source_start[event.get('source')]; if ss != nil then event.set('source_start', ss) end"
        }
        if ![ss] {
            grok {
                patterns_dir => ["patterns"]
                patterns_files_glob => "env-logs-patterns"
                match => { "source" => "-(?<dt>\d{8}-\d{6})\b" }
                tag_on_failure => [ "ss_dt_failure" ]
            }
            if [dt] {
                date {
                    locale => "en"
                    match => ["dt", "YYYYMMdd'-'HHmmss" ]
                    timezone => "Europe/Moscow"
                    target => "source_start"
                    remove_field => [ "dt" ]
                    tag_on_failure => [ "ss_ts_failure" ]
                }
            } else {
                ruby {
                    code => "event.set('source_start', LogStash::Timestamp.now)"
                }
            }
            ruby {
                code => "@@source_start[event.get('source')] = event.get('source_start')"
            }
        }
        if [ss] {
            ruby {
                code => "event.timestamp = LogStash::Timestamp.at(event.get('source_start').time.to_f + event.get('uptime'))"
            }
        }
    }

    # Fix log level
    grok {
        patterns_dir => ["/etc/logstash/patterns"]
        patterns_files_glob => "env-logs-patterns"
        match => { "[message]" => "^Logging at %{LOGLEVEL:level} level without checking if %{LOGLEVEL:[@metadata][level]} level is enabled: (?m)%{GREEDYDATA:message}" }
        overwrite => [ "level", "message" ]
        tag_on_failure => []
    }
    
    # Default log level INFO
    if ![level] {
        mutate {
            add_field => { "level" => "INFO" }
        }
    }
   
    # Parsing thread name, grid name and grid host
    if [message_kind] == "log" or [message_kind] == "td" {

        # Parsing thread name to prefix, number and grid name
        grok {
            patterns_dir => ["/etc/logstash/patterns"]
            patterns_files_glob => "env-logs-patterns"
            match => { "[thread][name]" => "^%{DATA:[thread][prefix]}-#%{INT:[thread][number]:int}\%%{DATA:[grid][name]}\%$" }
            tag_on_failure => [ "thread_failure" ]
        }
    
        # Try to parse grid name from message
        if ![grid][name] {
            grok {
                patterns_dir => ["/etc/logstash/patterns"]
                patterns_files_glob => "env-logs-patterns"
                match => { "[message]" => "Ignite.*?instance\s*name\s*[:=]\s*%{DATA:[grid][name]}([\s,\)\]]|$)" }
                tag_on_failure => [ "grid_name_failure" ]
            }
        }
        
        if ![grid][host] {
            # Extracting grid host from grid name
            grok {
                patterns_dir => ["/etc/logstash/patterns"]
                patterns_files_glob => "env-logs-patterns"
                match => { "[grid][name]" => "^%{DATA:[grid][name]}\%%{GRID_HOST:[grid][host]}$" }
                remove_tag => [ "grid_host_failure" ]
                tag_on_failure => [ "grid_host_failure" ]
            }
        }

        if ![grid][host] {
            # Try to parse grid host from message
            grok {
                patterns_dir => ["/etc/logstash/patterns"]
                patterns_files_glob => "env-logs-patterns"
                match => { "message" => "host\s+name:\s+%{WORD:[grid][host]}" }
                remove_tag => [ "grid_host_failure" ]
                tag_on_failure => [ "grid_host_failure" ]
            }
        }
        
        if [grid][host] {
        
            # Remember grid host
            ruby {
                init => "@@source_grid_host = {}"
                code => "@@source_grid_host[event.get('source')] = event.get('[grid][host]')"
            }
        } else {
 
            # Try to restore grid host from cache
            ruby {
                code => "i = 0; host = nil; while host == nil && i < 10 do if i > 0 then sleep(0.01) end; host = @@source_grid_host[event.get('source')]; i += 1 end; if host != nil then event.set('[grid][host]', host) else event.tag('source_grid_host_failure') end"
                remove_tag => [ "grid_host_failure" ]
            }
        }
    }
        
    # Parsing default log specific messages
    if [message_kind] == "log" {

        if ![message_type] {
            grok {
                patterns_dir => ["/etc/logstash/patterns"]
                patterns_files_glob => "env-logs-patterns"
                match => { "message" => "^Topology snapshot \[ver=%{NONNEGINT:[top][ver]:int}, servers=%{NONNEGINT:[top][servers]:int}, clients=%{NONNEGINT:[top][clients]:int}, CPUs=%{NONNEGINT:[top][cpu_count]:int}, (offheap=%{BASE10NUM:[top][offheap_size_gb]:float}GB, )?heap=%{BASE10NUM:[top][heap_size_gb]:float}GB\]" }
                add_field => { "message_type" => "top_snap" }
                tag_on_failure => []
            }
        }

        if ![message_type] {
            grok {
                patterns_dir => ["/etc/logstash/patterns"]
                patterns_files_glob => "env-logs-patterns"
                match => { "message" => "^Metrics for local node" }
                add_field => { "message_type" => "metrics" }
                tag_on_failure => []
            }

            if [message_type] == "metrics" {
                grok {
                    patterns_dir => ["/etc/logstash/patterns"]
                    patterns_files_glob => "env-logs-patterns"
                    match => { "message" => "Node \[id=%{DATA:[node][short_id]}, %{DATA:[node][name]}, uptime=%{DATA:[node][uptime]}\]" }
                    tag_on_failure => []
                }

                grok {
                    patterns_dir => ["/etc/logstash/patterns"]
                    patterns_files_glob => "env-logs-patterns"
                    match => { "message" => "H/N/C \[hosts=%{NONNEGINT:[top][hosts]:int}, nodes=%{NONNEGINT:[top][nodes]:int}, CPUs=%{NONNEGINT:[top][cpu_count]:int}\]" }
                    tag_on_failure => []
                }

                grok {
                    patterns_dir => ["/etc/logstash/patterns"]
                    patterns_files_glob => "env-logs-patterns"
                    match => { "message" => "CPU \[cur=%{BASE10NUM:[cpu][current]:float}\%, avg=%{BASE10NUM:[cpu][avg]:float}\%, GC=%{BASE10NUM:[cpu][gc]:float}\%\]" }
                    tag_on_failure => []
                }

                grok {
                    patterns_dir => ["/etc/logstash/patterns"]
                    patterns_files_glob => "env-logs-patterns"
                    match => { "message" => "PageMemory \[pages=%{NONNEGINT:[mem][pages]:int}\]" }
                    tag_on_failure => []
                }

                grok {
                    patterns_dir => ["/etc/logstash/patterns"]
                    patterns_files_glob => "env-logs-patterns"
                    match => { "message" => "Heap \[used=%{NONNEGINT:[heap][used_mb]:int}MB, free=%{BASE10NUM:[heap][free]:float}\%, comm=%{NONNEGINT:[heap][comm_mb]:int}MB\]" }
                    tag_on_failure => []
                }

                grok {
                    patterns_dir => ["/etc/logstash/patterns"]
                    patterns_files_glob => "env-logs-patterns"
                    match => { "message" => "Non heap \[used=%{NONNEGINT:[non_heap][used_mb]:int}MB, free=%{BASE10NUM:[non_heap][free]:float}\%, comm=%{NONNEGINT:[non_heap]:int}MB\]" }
                    tag_on_failure => []
                }

                grok {
                    patterns_dir => ["/etc/logstash/patterns"]
                    patterns_files_glob => "env-logs-patterns"
                    match => { "message" => "Public thread pool \[active=%{NONNEGINT:[thread_pool][pub][active]:int}, idle=%{NONNEGINT:[thread_pool][pub][idle]:int}, qSize=%{NONNEGINT:[thread_pool][pub][queue_size]:int}\]" }
                    tag_on_failure => []
                }

                grok {
                    patterns_dir => ["/etc/logstash/patterns"]
                    patterns_files_glob => "env-logs-patterns"
                    match => { "message" => "System thread pool \[active=%{NONNEGINT:[thread_pool][sys][active]:int}, idle=%{NONNEGINT:[thread_pool][sys][idle]:int}, qSize=%{NONNEGINT:[thread_pool][sys][queue_size]:int}\]" }
                    tag_on_failure => []
                }

                grok {
                    patterns_dir => ["/etc/logstash/patterns"]
                    patterns_files_glob => "env-logs-patterns"
                    match => { "message" => "Outbound messages queue \[size=%{NONNEGINT:[outbound_msg_queue][size]:int}\]" }
                    tag_on_failure => []
                }
            }
        }

        if ![message_type] {
            grok {
                patterns_dir => ["/etc/logstash/patterns"]
                patterns_files_glob => "env-logs-patterns"
                match => { "message" => "^Checkpoint started \[checkpointId=%{UUID:[cp][id]}, startPtr=FileWALPointer \[idx=%{NONNEGINT:[wal][idx]:int}, fileOff(set)?=%{NONNEGINT:[wal][off]:int}, len=%{NONNEGINT:[wal][len]:int}(, forceFlush=%{WORD:[wal][force_flush]})?\], checkpointLockWait=%{NONNEGINT:[cp][lock_wait_time]:int}ms, checkpointLockHoldTime=%{NONNEGINT:[cp][lock_hold_time]:int}ms, pages=%{NONNEGINT:[cp][pages]:int}, reason='%{DATA:[cp][reason]}'\]" }
                add_field => { "message_type" => "cp_start" }
                tag_on_failure => []
            }
        }

        if ![message_type] {
            grok {
                patterns_dir => ["/etc/logstash/patterns"]
                patterns_files_glob => "env-logs-patterns"
                match => { "message" => "^Checkpoint finished \[cpId=%{UUID:[cp][id]}, pages=%{NONNEGINT:[cp][pages]:int}, markPos=FileWALPointer \[idx=%{NONNEGINT:[wal][idx]:int}, fileOff(set)?=%{NONNEGINT:[wal][off]:int}, len=%{NONNEGINT:[wal][len]:int}(, forceFlush=%{WORD:[wal][force_flush]})?\](, walSegmentsCleared=%{NONNEGINT:[wal][segs_cleared]:int})?(, walHistorySize=%{NONNEGINT:[wal][hist_size]:int})?(, markBegin=%{NONNEGINT:[cp][mark_begin_time]:int}ms)?(, markDuration=%{NONNEGINT:[cp][mark_begin_time]:int}ms)?, pagesWrite=%{NONNEGINT:[cp][pages_write_time]:int}ms, fsync=%{NONNEGINT:[cp][fsync_time]:int}ms(, markEnd=%{NONNEGINT:[cp][mark_end_time]:int}ms)?, total=%{NONNEGINT:[cp][total_time]:int}ms\]" }
                add_field => { "message_type" => "cp_finish" }
                tag_on_failure => []
            }
            if [message_type] == "cp_finish" and [cp][total_time] > 0 {
                ruby {
                    code => "event.set('[cp][throughput]', event.get('[cp][pages]') * 1000 / event.get('[cp][total_time]'))"
                }
            }
        }

        if ![message_type] {
            grok {
                patterns_dir => ["/etc/logstash/patterns"]
                patterns_files_glob => "env-logs-patterns"
                match => { "message" => "^Skipping checkpoint.*?\[checkpointLockWait=%{NONNEGINT:[cp][lock_wait_time]:int}ms, checkpointLockHoldTime=%{NONNEGINT:[cp][lock_hold_time]:int}ms, reason='%{DATA:[cp][reason]}'\]" }
                add_field => { "message_type" => "cp_skip" }
                tag_on_failure => []
            }
        }

        if ![message_type] {
            grok {
                patterns_dir => ["/etc/logstash/patterns"]
                patterns_files_glob => "env-logs-patterns"
                match => { "message" => "^Could not clear historyMap due to WAL reservation on cpEntry %{UUID:[cp][entry]}, history map size is %{NONNEGINT:[historyMap][size]:int}" }
                add_field => { "message_type" => "cp_wal_reservation" }
                tag_on_failure => []
            }
        }

        if ![message_type] {
            grok {
                patterns_dir => ["/etc/logstash/patterns"]
                patterns_files_glob => "env-logs-patterns"
                match => { "message" => "^Checkpoint page buffer size is too big, setting to an adjusted cache size \[size=%{DATA:[mem][cp_buff][size]},  memPlc=%{DATA:[mem][policy][name]}\]" }
                add_field => { "message_type" => "cp_page_buff_size" }
                tag_on_failure => []
            }
        }

        if ![message_type] {
            grok {
                patterns_dir => ["/etc/logstash/patterns"]
                patterns_files_glob => "env-logs-patterns"
                match => { "message" => "^Read checkpoint status" }
                add_field => { "message_type" => "cp_status_read" }
                tag_on_failure => []
            }
        }

        if ![message_type] {
            grok {
                patterns_dir => ["/etc/logstash/patterns"]
                patterns_files_glob => "env-logs-patterns"
                match => { "message" => "^Started page memory \[memoryAllocated=%{DATA:[mem][total][size]}, pages=%{NONNEGINT:[mem][total][pages]:int}, tableSize=%{DATA:[mem][table][size]}, checkpointBuffer=%{DATA:[mem][cp_buff][size]}\]" }
                add_field => { "message_type" => "pgmem_started" }
                tag_on_failure => []
            }
        }

        if ![message_type] {
            grok {
                patterns_dir => ["/etc/logstash/patterns"]
                patterns_files_glob => "env-logs-patterns"
                match => { "message" => "^Checking memory state" }
                add_field => { "message_type" => "pgmem_check" }
                tag_on_failure => []
            }
        }

        if ![message_type] {
            grok {
                patterns_dir => ["/etc/logstash/patterns"]
                patterns_files_glob => "env-logs-patterns"
                match => { "message" => "^Finished applying memory changes" }
                add_field => { "message_type" => "pgmem_restored" }
                tag_on_failure => []
            }
        }

        if ![message_type] {
            grok {
                patterns_dir => ["/etc/logstash/patterns"]
                patterns_files_glob => "env-logs-patterns"
                match => { "message" => "^IO error encountered while running WAL flush" }
                match => { "message" => "^Failed to flush write-ahead log on cache stop" }
                add_field => { "message_type" => "wal_flush_failed" }
                tag_on_failure => []
            }
        }

        if ![message_type] {
            grok {
                patterns_dir => ["/etc/logstash/patterns"]
                patterns_files_glob => "env-logs-patterns"
                match => { "message" => "^Failed to gracefully close WAL segment" }
                add_field => { "message_type" => "wal_close_failed" }
                tag_on_failure => []
            }
        }

        if ![message_type] {
            grok {
                patterns_dir => ["/etc/logstash/patterns"]
                patterns_files_glob => "env-logs-patterns"
                match => { "message" => "^Stopping WAL iteration due to an exception" }
                add_field => { "message_type" => "wal_read_failed" }
                tag_on_failure => []
            }
        }

        if ![message_type] {
            grok {
                patterns_dir => ["/etc/logstash/patterns"]
                patterns_files_glob => "env-logs-patterns"
                match => { "message" => "^Ignite node stopped in the middle of checkpoint" }
                add_field => { "message_type" => "wal_cp_middle" }
                tag_on_failure => []
            }
        }

        if ![message_type] {
            grok {
                patterns_dir => ["/etc/logstash/patterns"]
                patterns_files_glob => "env-logs-patterns"
                match => { "message" => "^Found last checkpoint marker \[cpId=%{UUID:[cp][id]}," }
                add_field => { "message_type" => "wal_cp_found" }
                tag_on_failure => []
            }
        }

        if ![message_type] {
            grok {
                patterns_dir => ["/etc/logstash/patterns"]
                patterns_files_glob => "env-logs-patterns"
                match => { "message" => "^Applying lost cache updates since last checkpoint record" }
                add_field => { "message_type" => "wal_applying_lost_updates" }
                tag_on_failure => []
            }
        }

        if ![message_type] {
            grok {
                patterns_dir => ["/etc/logstash/patterns"]
                patterns_files_glob => "env-logs-patterns"
                match => { "message" => "^Finished applying WAL changes \[updatesApplied=%{NONNEGINT:[wal][apply][count]}, time=%{NONNEGINT:[wal][apply][time]}ms\]" }
                add_field => { "message_type" => "wal_apply_lost_updates_finished" }
                tag_on_failure => []
            }
        }

        if ![message_type] {
            grok {
                patterns_dir => ["/etc/logstash/patterns"]
                patterns_files_glob => "env-logs-patterns"
                match => { "message" => "^Started write-ahead log manager \[mode=%{WORD:[wal][mode]}\]" }
                add_field => { "message_type" => "wal_started" }
                tag_on_failure => []
            }
        }

        if ![message_type] {
            grok {
                patterns_dir => ["/etc/logstash/patterns"]
                patterns_files_glob => "env-logs-patterns"
                match => { "message" => "^Resuming logging to WAL segment" }
                add_field => { "message_type" => "wal_seg_resume" }
                tag_on_failure => []
            }
        }

        if ![message_type] {
            grok {
                patterns_dir => ["/etc/logstash/patterns"]
                patterns_files_glob => "env-logs-patterns"
                match => { "message" => "^Remove wal segments on snapshot delete snpId=%{NONNEGINT:[ss][id]:int} \[lowIdx=%{INT:[wal][seg][lowIdx]:int} - highIdx=%{INT:[wal][seg][highIdx]:int}\(not included\)\]" }
                add_field => { "message_type" => "wal_seg_remove_ss_delete" }
                tag_on_failure => []
            }
        }

        if ![message_type] {
            grok {
                patterns_dir => ["/etc/logstash/patterns"]
                patterns_files_glob => "env-logs-patterns"
                match => { "message" => "^No partition mapping found for cache during snapshot restore" }
                add_field => { "message_type" => "ss_restore_no_part_map" }
                tag_on_failure => []
            }
        }

        if ![message_type] {
            grok {
                patterns_dir => ["/etc/logstash/patterns"]
                patterns_files_glob => "env-logs-patterns"
                match => { "message" => "^Default mode is not possible on node" }
                add_field => { "message_type" => "ss_restore_not_dflt_mode" }
                tag_on_failure => []
            }
        }

        if ![message_type] {
            grok {
                patterns_dir => ["/etc/logstash/patterns"]
                patterns_files_glob => "env-logs-patterns"
                match => { "message" => "^No data for partition" }
                add_field => { "message_type" => "ss_no_part_data" }
                tag_on_failure => []
            }
        }

        if ![message_type] {
            grok {
                patterns_dir => ["/etc/logstash/patterns"]
                patterns_files_glob => "env-logs-patterns"
                match => { "message" => "^Handling node left during snapshot operation, node = %{DISCO_NODE}" }
                add_field => { "message_type" => "ss_node_left" }
                tag_on_failure => []
            }
        }

        if ![message_type] {
            grok {
                patterns_dir => ["/etc/logstash/patterns"]
                patterns_files_glob => "env-logs-patterns"
                match => { "message" => "^Can't read snapshot metadata for snapshot with id: %{NONNEGINT:[ss][id]:int}" }
                add_field => { "message_type" => "ss_read_meta_failed" }
                tag_on_failure => []
            }
        }

        if ![message_type] {
            grok {
                patterns_dir => ["/etc/logstash/patterns"]
                patterns_files_glob => "env-logs-patterns"
                match => { "message" => "^Error during snapshot operation" }
                match => { "message" => "^Snapshot operation was locally failed" }
                add_field => { "message_type" => "ss_op_failed" }
                tag_on_failure => []
            }
        }

        if ![message_type] {
            grok {
                patterns_dir => ["/etc/logstash/patterns"]
                patterns_files_glob => "env-logs-patterns"
                match => { "message" => "^Error occur while %{WORD:[ss][op][type]} snapshot operation with id = %{NONNEGINT:[ss][id]:int}" }
                add_field => { "message_type" => "ss_op_failed" }
                tag_on_failure => []
            }
        }

        if ![message_type] {
            grok {
                patterns_dir => ["/etc/logstash/patterns"]
                patterns_files_glob => "env-logs-patterns"
                match => { "message" => "^Error while starting snapshot operation" }
                add_field => { "message_type" => "ss_op_failed" }
                tag_on_failure => []
            }
        }

        if ![message_type] {
            grok {
                patterns_dir => ["/etc/logstash/patterns"]
                patterns_files_glob => "env-logs-patterns"
                match => { "message" => "^Failing snapshot operation future" }
                add_field => { "message_type" => "ss_op_failed" }
                tag_on_failure => []
            }
        }

        if ![message_type] {
            grok {
                patterns_dir => ["/etc/logstash/patterns"]
                patterns_files_glob => "env-logs-patterns"
                match => { "message" => "^Snapshot operation %{DATA:[ss][op][type]} started at %{DATA:[ss][start_date]} \[snapshotId=%{NONNEGINT:[ss][id]:int}," }
                match => { "message" => "^Snapshot worker started new snapshot operation: GridSnapshotOperationImpl{type=%{DATA:[ss][op][type]}, snapshotId=%{NONNEGINT:[ss][id]:int}," }
                add_field => { "message_type" => "ss_op_started" }
                tag_on_failure => []
            }
        }

        if ![message_type] {
            grok {
                patterns_dir => ["/etc/logstash/patterns"]
                patterns_files_glob => "env-logs-patterns"
                match => { "message" => "^Finished %{DATA:[ss][op][stage]} stage of snapshot operation %{DATA:[ss][op][type]} at %{DATA:[ss][stage_date]} \[snapshotId=%{NONNEGINT:[ss][id]:int}," }
                add_field => { "message_type" => "ss_stage_finished" }
                tag_on_failure => []
            }
        }

        if ![message_type] {
            grok {
                patterns_dir => ["/etc/logstash/patterns"]
                patterns_files_glob => "env-logs-patterns"
                match => { "message" => "^Finished snapshot operation %{DATA:[ss][op][type]} at %{DATA:[ss][start_date]} \[snapshotId=%{NONNEGINT:[ss][id]:int}," }
                match => { "message" => "^Snapshot finished in" }
                add_field => { "message_type" => "ss_op_finished" }
                tag_on_failure => []
            }
        }

        if ![message_type] {
            grok {
                patterns_dir => ["/etc/logstash/patterns"]
                patterns_files_glob => "env-logs-patterns"
                match => { "message" => "^Resolved snapshot directory: %{DATA:[ss][dir]}" }
                add_field => { "message_type" => "ss_dir_resolved" }
                tag_on_failure => []
            }
        }

        if ![message_type] {
            grok {
                patterns_dir => ["/etc/logstash/patterns"]
                patterns_files_glob => "env-logs-patterns"
                match => { "message" => "^Snapshot initialization completed \[topVer=%{TOP_VER}, time=%{NONNEGINT:[ss][init][time]}ms\]" }
                add_field => { "message_type" => "ss_init_completed" }
                tag_on_failure => []
            }
        }

        if ![message_type] {
            grok {
                patterns_dir => ["/etc/logstash/patterns"]
                patterns_files_glob => "env-logs-patterns"
                match => { "message" => "^Received message with wrong id, msg = %{WORD:[ss][msg][type]}" }
                add_field => { "message_type" => "ss_wrong_msg_id" }
                tag_on_failure => []
            }
        }

        if ![message_type] {
            grok {
                patterns_dir => ["/etc/logstash/patterns"]
                patterns_files_glob => "env-logs-patterns"
                match => { "message" => "^Duplicate message from node" }
                add_field => { "message_type" => "ss_dup_msg" }
                tag_on_failure => []
            }
        }

        if ![message_type] {
            grok {
                patterns_dir => ["/etc/logstash/patterns"]
                patterns_files_glob => "env-logs-patterns"
                match => { "message" => "^Cancel snapshot operation" }
                add_field => { "message_type" => "ss_op_cancel" }
                tag_on_failure => []
            }
        }

        if ![message_type] {
            grok {
                patterns_dir => ["/etc/logstash/patterns"]
                patterns_files_glob => "env-logs-patterns"
                match => { "message" => "^Create snapshot request received \[snapshotId=%{NONNEGINT:[ss][id]:int}," }
                add_field => { "message_type" => "ss_create" }
                tag_on_failure => []
            }
        }

        if ![message_type] {
            grok {
                patterns_dir => ["/etc/logstash/patterns"]
                patterns_files_glob => "env-logs-patterns"
                match => { "message" => "^Received request to collect snapshot list from cluster" }
                match => { "message" => "^Collect list of snapshot schedules" }
                match => { "message" => "^(%{WORD}\s+)*(GetSnapshotList|CollectSnasphotListJob|CollectSnapshotListJob|CollectSnapshotInfoJob|CollectSnapshotInfoTaskResult|CollectSnapshotDescriptorTask)" }
                add_field => { "message_type" => "ss_collect" }
                tag_on_failure => []
            }
        }

        if ![message_type] {
            grok {
                patterns_dir => ["/etc/logstash/patterns"]
                patterns_files_glob => "env-logs-patterns"
                match => { "message" => "^Check snapshot request received \[snapshotId=%{NONNEGINT:[ss][id]:int}," }
                add_field => { "message_type" => "ss_check" }
                tag_on_failure => []
            }
        }

        if ![message_type] {
            grok {
                patterns_dir => ["/etc/logstash/patterns"]
                patterns_files_glob => "env-logs-patterns"
                match => { "message" => "^Move snapshot request received \[snapshotId=%{NONNEGINT:[ss][id]:int}," }
                add_field => { "message_type" => "ss_move" }
                tag_on_failure => []
            }
        }

        if ![message_type] {
            grok {
                patterns_dir => ["/etc/logstash/patterns"]
                patterns_files_glob => "env-logs-patterns"
                match => { "message" => "^Delete snapshot request received \[snapshotId=%{NONNEGINT:[ss][id]:int}," }
                add_field => { "message_type" => "ss_move" }
                tag_on_failure => []
            }
        }

        if ![message_type] {
            grok {
                patterns_dir => ["/etc/logstash/patterns"]
                patterns_files_glob => "env-logs-patterns"
                match => { "message" => "^Restore snapshot request received \[snapshotId=%{NONNEGINT:[ss][id]:int}," }
                match => { "message" => "^Received info from cluster for id = %{NONNEGINT:[ss][id]:int}," }
                match => { "message" => "^Restoring all caches from affected cache groups: \[snapshotId=%{NONNEGINT:[ss][id]:int}," }
                add_field => { "message_type" => "ss_restore" }
                tag_on_failure => []
            }
        }

        if ![message_type] {
            grok {
                patterns_dir => ["/etc/logstash/patterns"]
                patterns_files_glob => "env-logs-patterns"
                match => { "message" => "^Received snapshot metadata from the cluster: SnapshotInfoImpl{snapshotId=%{NONNEGINT:[ss][id]:int}," }
                add_field => { "message_type" => "ss_meta" }
                tag_on_failure => []
            }
        }

        if ![message_type] {
            grok {
                patterns_dir => ["/etc/logstash/patterns"]
                patterns_files_glob => "env-logs-patterns"
                match => { "message" => "^Will not include SQL indexes to snapshot" }
                add_field => { "message_type" => "ss_idx_not_include" }
                tag_on_failure => []
            }
        }

        if ![message_type] {
            grok {
                patterns_dir => ["/etc/logstash/patterns"]
                patterns_files_glob => "env-logs-patterns"
                match => { "message" => "^Get stream for snapshot \(id = %{NONNEGINT:[ss][id]:int}" }
                add_field => { "message_type" => "ss_get_stream" }
                tag_on_failure => []
            }
        }

        if ![message_type] {
            grok {
                patterns_dir => ["/etc/logstash/patterns"]
                patterns_files_glob => "env-logs-patterns"
                match => { "message" => "^Snapshots catalog processor started" }
                add_field => { "message_type" => "ss_catalog_proc_started" }
                tag_on_failure => []
            }
        }

        if ![message_type] {
            grok {
                patterns_dir => ["/etc/logstash/patterns"]
                patterns_files_glob => "env-logs-patterns"
                match => { "message" => "^Snapshots catalog processor stopped" }
                add_field => { "message_type" => "ss_catalog_proc_stopped" }
                tag_on_failure => []
            }
        }

        if ![message_type] {
            grok {
                patterns_dir => ["/etc/logstash/patterns"]
                patterns_files_glob => "env-logs-patterns"
                match => { "message" => "^Sending messages to snapshot catalog" }
                add_field => { "message_type" => "ss_catalog_msgs_sending" }
                tag_on_failure => []
            }
        }

        if ![message_type] {
            grok {
                patterns_dir => ["/etc/logstash/patterns"]
                patterns_files_glob => "env-logs-patterns"
                match => { "message" => "^Snapshot schedule processor awaits for cluster activation" }
                add_field => { "message_type" => "sss_wait_active" }
                tag_on_failure => []
            }
        }

        if ![message_type] {
            grok {
                patterns_dir => ["/etc/logstash/patterns"]
                patterns_files_glob => "env-logs-patterns"
                match => { "message" => "^Restart snapshot schedules" }
                add_field => { "message_type" => "sss_restart" }
                tag_on_failure => []
            }
        }

        if ![message_type] {
            grok {
                patterns_dir => ["/etc/logstash/patterns"]
                patterns_files_glob => "env-logs-patterns"
                match => { "message" => "^Failed to restart schedules" }
                add_field => { "message_type" => "sss_restart_failed" }
                tag_on_failure => []
            }
        }

        if ![message_type] {
            grok {
                patterns_dir => ["/etc/logstash/patterns"]
                patterns_files_glob => "env-logs-patterns"
                match => { "message" => "^Scheduled snapshot operation started" }
                add_field => { "message_type" => "sss_op_started" }
                tag_on_failure => []
            }
        }

        if ![message_type] {
            grok {
                patterns_dir => ["/etc/logstash/patterns"]
                patterns_files_glob => "env-logs-patterns"
                match => { "message" => "^Failed to run scheduled snapshot operation" }
                add_field => { "message_type" => "sss_run_failed" }
                tag_on_failure => []
            }
        }

        if ![message_type] {
            grok {
                patterns_dir => ["/etc/logstash/patterns"]
                patterns_files_glob => "env-logs-patterns"
                match => { "message" => "^Nothing to move" }
                add_field => { "message_type" => "sss_move_nothing" }
                tag_on_failure => []
            }
        }

        if ![message_type] {
            grok {
                patterns_dir => ["/etc/logstash/patterns"]
                patterns_files_glob => "env-logs-patterns"
                match => { "message" => "^Found outdated snapshots to move" }
                add_field => { "message_type" => "sss_move_outdated" }
                tag_on_failure => []
            }
        }

        if ![message_type] {
            grok {
                patterns_dir => ["/etc/logstash/patterns"]
                patterns_files_glob => "env-logs-patterns"
                match => { "message" => "^Start snapshot schedule: SnapshotSchedule \[id=%{UUID:[sss][id], name=%{DATA:[sss][name]}," }
                add_field => { "message_type" => "sss_start" }
                tag_on_failure => []
            }
        }

        if ![message_type] {
            grok {
                patterns_dir => ["/etc/logstash/patterns"]
                patterns_files_glob => "env-logs-patterns"
                match => { "message" => "^Delete snapshot schedule: %{DATA:[sss][name]}" }
                add_field => { "message_type" => "sss_delete" }
                tag_on_failure => []
            }
        }

        if ![message_type] {
            grok {
                patterns_dir => ["/etc/logstash/patterns"]
                patterns_files_glob => "env-logs-patterns"
                match => { "message" => "^Disable snapshot schedule: %{DATA:[sss][name]}" }
                add_field => { "message_type" => "sss_disable" }
                tag_on_failure => []
            }
        }

        if ![message_type] {
            grok {
                patterns_dir => ["/etc/logstash/patterns"]
                patterns_files_glob => "env-logs-patterns"
                match => { "message" => "^>>> Possible starvation in striped pool" }
                add_field => { "message_type" => "long_running_stripe" }
                tag_on_failure => []
            }
        }

        if ![message_type] {
            grok {
                patterns_dir => ["/etc/logstash/patterns"]
                patterns_files_glob => "env-logs-patterns"
                match => { "message" => "^Still waiting for a concurrent write to complete" }
                add_field => { "message_type" => "long_running_wal_write" }
                tag_on_failure => []
            }
        }

        if ![message_type] {
            grok {
                patterns_dir => ["/etc/logstash/patterns"]
                patterns_files_glob => "env-logs-patterns"
                match => { "message" => "^Found long running transaction" }
                add_field => { "message_type" => "long_running_tx" }
                tag_on_failure => []
            }
        }

        if ![message_type] {
            grok {
                patterns_dir => ["/etc/logstash/patterns"]
                patterns_files_glob => "env-logs-patterns"
                match => { "message" => "^Found long running cache future" }
                add_field => { "message_type" => "long_running_fut" }
                tag_on_failure => []
            }
        }

        if ![message_type] {
            grok {
                patterns_dir => ["/etc/logstash/patterns"]
                patterns_files_glob => "env-logs-patterns"
                match => { "message" => "^Query execution is too long" }
                add_field => { "message_type" => "long_running_query" }
                tag_on_failure => []
            }
        }

        if ![message_type] {
            grok {
                patterns_dir => ["/etc/logstash/patterns"]
                patterns_files_glob => "env-logs-patterns"
                match => { "message" => "^Waited too long to acquire topology write lock" }
                add_field => { "message_type" => "long_waiting_top_wlock" }
                tag_on_failure => []
            }
        }

        if ![message_type] {
            grok {
                patterns_dir => ["/etc/logstash/patterns"]
                patterns_files_glob => "env-logs-patterns"
                match => { "message" => "^Failed to prepare DHT transaction: %{WORD:[tx][type]}" }
                add_field => { "message_type" => "tx_prepare_failed" }
                tag_on_failure => []
            }
        }

        if ![message_type] {
            grok {
                patterns_dir => ["/etc/logstash/patterns"]
                patterns_files_glob => "env-logs-patterns"
                match => { "message" => "^Received finish request for completed transaction" }
                add_field => { "message_type" => "tx_finish_completed" }
                tag_on_failure => []
            }
        }

        if ![message_type] {
            grok {
                patterns_dir => ["/etc/logstash/patterns"]
                patterns_files_glob => "env-logs-patterns"
                match => { "message" => "^Failed to execute local query.\norg.apache.ignite.cache.query.QueryCancelledException: The query was cancelled while executing" }
                add_field => { "message_type" => "query_canceled" }
                tag_on_failure => []
            }
        }

        if ![message_type] {
            grok {
                patterns_dir => ["/etc/logstash/patterns"]
                patterns_files_glob => "env-logs-patterns"
                match => { "message" => "^(<%{DATA:[cache][name]}> )?Failed to run query" }
                add_field => { "message_type" => "query_run_failed" }
                tag_on_failure => []
            }
        }

        if ![message_type] {
            grok {
                patterns_dir => ["/etc/logstash/patterns"]
                patterns_files_glob => "env-logs-patterns"
                match => { "message" => "^Failed to execute local query" }
                add_field => { "message_type" => "query_failed" }
                tag_on_failure => []
            }
        }

        if ![message_type] {
            grok {
                patterns_dir => ["/etc/logstash/patterns"]
                patterns_files_glob => "env-logs-patterns"
                match => { "message" => "^Failed to send job request.*?\[node=%{DISCO_NODE}, taskName=%{DATA:[task][name]}," }
                match => { "message" => "^Failed to send job request" }
                add_field => { "message_type" => "job_send_failed" }
                tag_on_failure => []
            }
        }

        if ![message_type] {
            grok {
                patterns_dir => ["/etc/logstash/patterns"]
                patterns_files_glob => "env-logs-patterns"
                match => { "message" => "^Failed to reduce job results due to undeclared user exception \[task=%{DATA:[task][name]}, err=" }
                match => { "message" => "^Failed to reduce job results" }
                add_field => { "message_type" => "job_reduce_failed" }
                tag_on_failure => []
            }
        }

        if ![message_type] {
            grok {
                patterns_dir => ["/etc/logstash/patterns"]
                patterns_files_glob => "env-logs-patterns"
                match => { "message" => "^Failed to execute job" }
                add_field => { "message_type" => "job_exec_failed" }
                tag_on_failure => []
            }
        }

        if ![message_type] {
            grok {
                patterns_dir => ["/etc/logstash/patterns"]
                patterns_files_glob => "env-logs-patterns"
                match => { "message" => "^Will wait for all job responses from worker nodes before stopping grid" }
                add_field => { "message_type" => "job_wait_response" }
                tag_on_failure => []
            }
        }

        if ![message_type] {
            grok {
                patterns_dir => ["/etc/logstash/patterns"]
                patterns_files_glob => "env-logs-patterns"
                match => { "message" => "^Failed over job to a new node" }
                add_field => { "message_type" => "job_failover_failed" }
                tag_on_failure => []
            }
        }

        if ![message_type] {
            grok {
                patterns_dir => ["/etc/logstash/patterns"]
                patterns_files_glob => "env-logs-patterns"
                match => { "message" => "^Received job request message from unknown node" }
                add_field => { "message_type" => "job_unknown_node" }
                tag_on_failure => []
            }
        }

        if ![message_type] {
            grok {
                patterns_dir => ["/etc/logstash/patterns"]
                patterns_files_glob => "env-logs-patterns"
                match => { "message" => "^Received job execution response while stopping grid" }
                add_field => { "message_type" => "job_response_ignored" }
                tag_on_failure => []
            }
        }

        if ![message_type] {
            grok {
                patterns_dir => ["/etc/logstash/patterns"]
                patterns_files_glob => "env-logs-patterns"
                match => { "message" => "^Job is being cancelled because master task node left grid" }
                add_field => { "message_type" => "task_node_left" }
                tag_on_failure => []
            }
        }

        if ![message_type] {
            grok {
                patterns_dir => ["/etc/logstash/patterns"]
                patterns_files_glob => "env-logs-patterns"
                match => { "message" => "^Failed to obtain remote job result policy for result from ComputeTask\.result" }
                add_field => { "message_type" => "task_failed" }
                tag_on_failure => []
            }
        }

        if ![message_type] {
            grok {
                patterns_dir => ["/etc/logstash/patterns"]
                patterns_files_glob => "env-logs-patterns"
                match => { "message" => "^Failed to map task jobs to nodes.*?\[taskName=%{DATA:[task][name]}," }
                match => { "message" => "^Failed to map task jobs to nodes" }
                add_field => { "message_type" => "task_map_failed" }
                tag_on_failure => []
            }
        }

        if ![message_type] {
            grok {
                patterns_dir => ["/etc/logstash/patterns"]
                patterns_files_glob => "env-logs-patterns"
                match => { "message" => "^Failed to execute task \[name=%{DATA:[task][name]}, clientId=%{DATA:[node][id]}\]" }
                match => { "message" => "^Failed to execute task" }
                add_field => { "message_type" => "task_exec_failed" }
                tag_on_failure => []
            }
        }

        if ![message_type] {
            grok {
                patterns_dir => ["/etc/logstash/patterns"]
                patterns_files_glob => "env-logs-patterns"
                match => { "message" => "^Will cancel unfinished tasks due to stopping of the grid \[cnt=%{NONNEGINT:[task][count]}\]" }
                match => { "message" => "^Will cancel unfinished tasks" }
                add_field => { "message_type" => "task_cancel" }
                tag_on_failure => []
            }
        }

        if ![message_type] {
            grok {
                patterns_dir => ["/etc/logstash/patterns"]
                patterns_files_glob => "env-logs-patterns"
                match => { "message" => "^\[%{TASK_TYPE:[task][type]}\]:" }
                add_field => { "message_type" => "task_msg" }
                tag_on_failure => []
            }
        }

        if ![message_type] {
            grok {
                patterns_dir => ["/etc/logstash/patterns"]
                patterns_files_glob => "env-logs-patterns"
                match => { "message" => "^Starting %{JOB_TYPE:[job][type]}:" }
                add_field => { "message_type" => "job_starting" }
                tag_on_failure => []
            }
        }

        if ![message_type] {
            grok {
                patterns_dir => ["/etc/logstash/patterns"]
                patterns_files_glob => "env-logs-patterns"
                match => { "message" => "^\[%{JOB_TYPE:[job][type]}\]:" }
                add_field => { "message_type" => "job_msg" }
                tag_on_failure => []
            }
        }

        if ![message_type] {
            grok {
                patterns_dir => ["/etc/logstash/patterns"]
                patterns_files_glob => "env-logs-patterns"
                match => { "message" => "^Task locally deployed: %{DATA:[task][name]}" }
                match => { "message" => "^Task was deployed.*?: %{DATA:[task][name]}" }
                add_field => { "message_type" => "task_deployed" }
                tag_on_failure => []
            }
        }

        if ![message_type] {
            grok {
                patterns_dir => ["/etc/logstash/patterns"]
                patterns_files_glob => "env-logs-patterns"
                match => { "message" => "^Task was undeployed.*?\[cls=%{DATA:[task][name]}, alias=%{DATA:[task][alias]}\]" }
                add_field => { "message_type" => "task_undeployed" }
                tag_on_failure => []
            }
        }

        if ![message_type] {
            grok {
                patterns_dir => ["/etc/logstash/patterns"]
                patterns_files_glob => "env-logs-patterns"
                match => { "message" => "^Class locally deployed: %{DATA:[class][name]}" }
                match => { "message" => "^Class was deployed.*?: %{DATA:[class][name]}" }
                add_field => { "message_type" => "class_deployed" }
                tag_on_failure => []
            }
        }

        if ![message_type] {
            grok {
                patterns_dir => ["/etc/logstash/patterns"]
                patterns_files_glob => "env-logs-patterns"
                match => { "message" => "^Class was undeployed.*?\[cls=%{DATA:[class][name]}, alias=%{DATA:[class][alias]}\]" }
                add_field => { "message_type" => "class_undeployed" }
                tag_on_failure => []
            }
        }

        if ![message_type] {
            grok {
                patterns_dir => ["/etc/logstash/patterns"]
                patterns_files_glob => "env-logs-patterns"
                match => { "message" => "^Removed undeployed class.*?sampleClsName=%{DATA:[class][name]}," }
                match => { "message" => "^Removed undeployed class" }
                add_field => { "message_type" => "class_removed" }
                tag_on_failure => []
            }
        }

        if ![message_type] {
            grok {
                patterns_dir => ["/etc/logstash/patterns"]
                patterns_files_glob => "env-logs-patterns"
                match => { "message" => "^Rolling updates are not properly configured" }
                add_field => { "message_type" => "config_diff_rolling" }
                tag_on_failure => []
            }
        }

        if ![message_type] {
            grok {
                patterns_dir => ["/etc/logstash/patterns"]
                patterns_files_glob => "env-logs-patterns"
                match => { "message" => "^Local java version is different from remote" }
                add_field => { "message_type" => "config_diff_java" }
                tag_on_failure => []
            }
        }

        if ![message_type] {
            grok {
                patterns_dir => ["/etc/logstash/patterns"]
                patterns_files_glob => "env-logs-patterns"
                match => { "message" => "^Local node's value of 'java.net.preferIPv4Stack' system property differs from remote node's" }
                add_field => { "message_type" => "config_diff_ip4stack" }
                tag_on_failure => []
            }
        }

        if ![message_type] {
            grok {
                patterns_dir => ["/etc/logstash/patterns"]
                patterns_files_glob => "env-logs-patterns"
                match => { "message" => "^Local node's build version differs from remote node's" }
                add_field => { "message_type" => "config_diff_version" }
                tag_on_failure => []
            }
        }

        if ![message_type] {
            grok {
                patterns_dir => ["/etc/logstash/patterns"]
                patterns_files_glob => "env-logs-patterns"
                match => { "message" => "^Local node's binary configuration is not equal to remote node's binary configuration" }
                add_field => { "message_type" => "config_diff_binary" }
                tag_on_failure => []
            }
        }

        if ![message_type] {
            grok {
                patterns_dir => ["/etc/logstash/patterns"]
                patterns_files_glob => "env-logs-patterns"
                match => { "message" => "^Closed client session due to exception" }
                match => { "message" => "^Closing NIO session because of unhandled exception" }
                add_field => { "message_type" => "client_failed_closed" }
                tag_on_failure => []
            }
        }

        if ![message_type] {
            grok {
                patterns_dir => ["/etc/logstash/patterns"]
                patterns_files_glob => "env-logs-patterns"
                match => { "message" => "^Failed to process client request" }
                match => { "message" => "^Failed to handle request" }
                add_field => { "message_type" => "client_request_failed" }
                tag_on_failure => []
            }
        }

        if ![message_type] {
            grok {
                patterns_dir => ["/etc/logstash/patterns"]
                patterns_files_glob => "env-logs-patterns"
                match => { "message" => "^Client connector processor has started on TCP port %{NONNEGINT:[client][conn][port]:int}" }
                add_field => { "message_type" => "client_conn_started" }
                tag_on_failure => []
            }
        }

        if ![message_type] {
            grok {
                patterns_dir => ["/etc/logstash/patterns"]
                patterns_files_glob => "env-logs-patterns"
                match => { "message" => "^Command protocol successfully started \[name=%{DATA:[client][srv][name]}, host=%{DATA:[client][srv][addr]}, port=%{NONNEGINT:[client][srv][port]:int}\]" }
                add_field => { "message_type" => "client_srv_started" }
                tag_on_failure => []
            }
        }

        if ![message_type] {
            grok {
                patterns_dir => ["/etc/logstash/patterns"]
                patterns_files_glob => "env-logs-patterns"
                match => { "message" => "^Command protocol successfully stopped: %{DATA:[client][srv][name]}" }
                add_field => { "message_type" => "client_srv_started" }
                tag_on_failure => []
            }
        }

        if ![message_type] {
            grok {
                patterns_dir => ["/etc/logstash/patterns"]
                patterns_files_glob => "env-logs-patterns"
                match => { "message" => "^Runnable tasks outlived thread pool executor service" }
                add_field => { "message_type" => "runnable_outlived" }
                tag_on_failure => []
            }
        }

        if ![message_type] {
            grok {
                patterns_dir => ["/etc/logstash/patterns"]
                patterns_files_glob => "env-logs-patterns"
                match => { "message" => "^Runtime error caught during grid runnable execution" }
                match => { "message" => "^Failed to execute runnable" }
                add_field => { "message_type" => "runnable_failed" }
                tag_on_failure => []
            }
        }

        if ![message_type] {
            grok {
                patterns_dir => ["/etc/logstash/patterns"]
                patterns_files_glob => "env-logs-patterns"
                match => { "message" => "^(<%{DATA:[cache][name]}> )?Future execution resulted in error: %{WORD:[fut][type]}" }
                add_field => { "message_type" => "fut_exec_failed" }
                tag_on_failure => []
            }
        }

        if ![message_type] {
            grok {
                patterns_dir => ["/etc/logstash/patterns"]
                patterns_files_glob => "env-logs-patterns"
                match => { "message" => "^Failed to notify listener" }
                add_field => { "message_type" => "fut_notify_list_failed" }
                tag_on_failure => []
            }
        }

        if ![message_type] {
            grok {
                patterns_dir => ["/etc/logstash/patterns"]
                patterns_files_glob => "env-logs-patterns"
                match => { "message" => "^Failed to execute compound future reducer" }
                add_field => { "message_type" => "fut_exec_rdc__failed" }
                tag_on_failure => []
            }
        }

        if ![message_type] {
            grok {
                patterns_dir => ["/etc/logstash/patterns"]
                patterns_files_glob => "env-logs-patterns"
                match => { "message" => "^%{FUTURE_TYPE:[fut][type]} waiting for response.*?node=%{UUID:[node][id]}.*?topVer=%{TOP_VER}" }
                match => { "message" => "^%{FUTURE_TYPE:[fut][type]} waiting for response" }
                add_field => { "message_type" => "fut_waiting" }
                tag_on_failure => []
            }
        }

        if ![message_type] {
            grok {
                patterns_dir => ["/etc/logstash/patterns"]
                patterns_files_glob => "env-logs-patterns"
                match => { "message" => "^(<%{DATA:[cache][name]}> )?Unexpected exception during cache update" }
                add_field => { "message_type" => "update_failed" }
                tag_on_failure => []
            }
        }

        if ![message_type] {
            grok {
                patterns_dir => ["/etc/logstash/patterns"]
                patterns_files_glob => "env-logs-patterns"
                match => { "message" => "^(<%{DATA:[cache][name]}> )?Failed to update key on backup" }
                add_field => { "message_type" => "update_backup_failed" }
                tag_on_failure => []
            }
        }

        if ![message_type] {
            grok {
                patterns_dir => ["/etc/logstash/patterns"]
                patterns_files_glob => "env-logs-patterns"
                match => { "message" => "^Error when executing timeout callback" }
                add_field => { "message_type" => "timeout_exec_failed" }
                tag_on_failure => []
            }
        }

        if ![message_type] {
            grok {
                patterns_dir => ["/etc/logstash/patterns"]
                patterns_files_glob => "env-logs-patterns"
                match => { "message" => "^Set transaction invalidation flag to true due to error \[tx=%{WORD:[tx][type]}" }
                add_field => { "message_type" => "tx_failed" }
                tag_on_failure => []
            }
        }

        if ![message_type] {
            grok {
                patterns_dir => ["/etc/logstash/patterns"]
                patterns_files_glob => "env-logs-patterns"
                match => { "message" => "Cache transaction timed out: %{WORD:[tx][type]}" }
                add_field => { "message_type" => "tx_timeout" }
                tag_on_failure => []
            }
        }

        if ![message_type] {
            grok {
                patterns_dir => ["/etc/logstash/patterns"]
                patterns_files_glob => "env-logs-patterns"
                match => { "message" => "Failed to acquire lock within provided timeout for transaction \[timeout=%{NONNEGINT:[tx][timeout]}, tx=%{DATA:[tx][type]}\]" }
                match => { "message" => "Failed to acquire lock within provided timeout for transaction" }
                add_field => { "message_type" => "tx_lock_timeout" }
                tag_on_failure => []
            }
        }

        if ![message_type] {
            grok {
                patterns_dir => ["/etc/logstash/patterns"]
                patterns_files_glob => "env-logs-patterns"
                match => { "message" => "^Transaction was rolled back because the timeout is reached: %{WORD:[tx][type]}" }
                add_field => { "message_type" => "tx_timeout_rolledback" }
                tag_on_failure => []
            }
        }

        if ![message_type] {
            grok {
                patterns_dir => ["/etc/logstash/patterns"]
                patterns_files_glob => "env-logs-patterns"
                match => { "message" => "^Heuristic transaction failure" }
                add_field => { "message_type" => "tx_heuristic_failure" }
                tag_on_failure => []
            }
        }

        if ![message_type] {
            grok {
                patterns_dir => ["/etc/logstash/patterns"]
                patterns_files_glob => "env-logs-patterns"
                match => { "message" => "^Failed to finish transaction \[commit=%{WORD:[tx][commit]}, tx=%{WORD:[tx][type]}" }
                add_field => { "message_type" => "tx_finish_failed" }
                tag_on_failure => []
            }
        }

        if ![message_type] {
            grok {
                patterns_dir => ["/etc/logstash/patterns"]
                patterns_files_glob => "env-logs-patterns"
                match => { "message" => "^Failed to commit transaction, node is stopping \[tx=%{WORD:[tx][type]}" }
                add_field => { "message_type" => "tx_commit_failed" }
                tag_on_failure => []
            }
        }

        if ![message_type] {
            grok {
                patterns_dir => ["/etc/logstash/patterns"]
                patterns_files_glob => "env-logs-patterns"
                match => { "message" => "^(<%{DATA:[cache][name]}> )?Failed processing get request: %{WORD:[request][type]}" }
                add_field => { "message_type" => "request_get_failed" }
                tag_on_failure => []
            }
        }

        if ![message_type] {
            grok {
                patterns_dir => ["/etc/logstash/patterns"]
                patterns_files_glob => "env-logs-patterns"
                match => { "message" => "^(<%{DATA:[cache][name]}> )?Failed to acquire lock for request: %{WORD:[request][type]}" }
                add_field => { "message_type" => "request_lock_failed" }
                tag_on_failure => []
            }
        }

        if ![message_type] {
            grok {
                patterns_dir => ["/etc/logstash/patterns"]
                patterns_files_glob => "env-logs-patterns"
                match => { "message" => "Failed to send near lock response.*?\[txId=%{DATA:[tx][id]}, inTx=%{WORD:[inTx]}, node=%{UUID:[node][id]}, res=%{WORD:[resp][type]}" }
                add_field => { "message_type" => "response_lock_failed" }
                tag_on_failure => []
            }
        }

        if ![message_type] {
            grok {
                patterns_dir => ["/etc/logstash/patterns"]
                patterns_files_glob => "env-logs-patterns"
                match => { "message" => "^(<%{DATA:[cache][name]}> )?Failed to send get response to node.*?\[nodeId=%{UUID:[node][id]},req=%{WORD:[request][type]}" }
                add_field => { "message_type" => "response_get_failed" }
                tag_on_failure => []
            }
        }

        if ![message_type] {
            grok {
                patterns_dir => ["/etc/logstash/patterns"]
                patterns_files_glob => "env-logs-patterns"
                match => { "message" => "^Failed to connect to any address from IP finder" }
                add_field => { "message_type" => "disco_connect_any_failed" }
                tag_on_failure => []
            }
        }

        if ![message_type] {
            grok {
                patterns_dir => ["/etc/logstash/patterns"]
                patterns_files_glob => "env-logs-patterns"
                match => { "message" => "^Node has not been connected to topology and will repeat join process" }
                add_field => { "message_type" => "disco_not_connected" }
                tag_on_failure => []
            }
        }

        if ![message_type] {
            grok {
                patterns_dir => ["/etc/logstash/patterns"]
                patterns_files_glob => "env-logs-patterns"
                match => { "message" => "^Failed to send message to next node \[msg=%{WORD:[msg][type]}" }
                add_field => { "message_type" => "disco_msg_send_failed" }
                tag_on_failure => []
            }
        }

        if ![message_type] {
            grok {
                patterns_dir => ["/etc/logstash/patterns"]
                patterns_files_glob => "env-logs-patterns"
                match => { "message" => "^Socket write has timed out.*?\[failureDetectionTimeout=%{NONNEGINT:failureDetectionTimeout:int}, rmtAddr=%{DATA:[node][addr]}, rmtPort=%{NONNEGINT:[node][port]:int}, sockTimeout=%{NONNEGINT:[disco][sock_timeout]:int}\]" }
                add_field => { "message_type" => "disco_write_timeout" }
                tag_on_failure => []
            }
        }

        if ![message_type] {
            grok {
                patterns_dir => ["/etc/logstash/patterns"]
                patterns_files_glob => "env-logs-patterns"
                match => { "message" => "^Timed out waiting for message to be read \(most probably, the reason is long GC pauses on remote node\) \[curTimeout=%{NONNEGINT:currentTimeout:int}, rmtAddr=rmtAddr=%{DATA:[node][addr]}, rmtPort=%{NONNEGINT:[node][port]:int}\]" }
                add_field => { "message_type" => "disco_read_timeout" }
                tag_on_failure => []
            }
        }

        if ![message_type] {
            grok {
                patterns_dir => ["/etc/logstash/patterns"]
                patterns_files_glob => "env-logs-patterns"
                match => { "message" => "^Socket operation timed out on handshake \(consider increasing 'networkTimeout' configuration property\) \[netTimeout=%{NONNEGINT:networkTimeout:int}\]" }
                add_field => { "message_type" => "disco_handshake_timeout" }
                tag_on_failure => []
            }
        }

        if ![message_type] {
            grok {
                patterns_dir => ["/etc/logstash/patterns"]
                patterns_files_glob => "env-logs-patterns"
                match => { "message" => "^Timed out waiting for message delivery receipt" }
                add_field => { "message_type" => "disco_receipt_timeout" }
                tag_on_failure => []
            }
        }

        if ![message_type] {
            grok {
                patterns_dir => ["/etc/logstash/patterns"]
                patterns_files_glob => "env-logs-patterns"
                match => { "message" => "^Failing client node due to not receiving metrics updates from client node within.*?\[timeout=%{NONNEGINT:[disco][client_timeout]:int}, node=%{DISCO_NODE}\]" }
                add_field => { "message_type" => "disco_client_timeout" }
                tag_on_failure => []
            }
        }

        if ![message_type] {
            grok {
                patterns_dir => ["/etc/logstash/patterns"]
                patterns_files_glob => "env-logs-patterns"
                match => { "message" => "^Local node has detected failed nodes and started cluster-wide procedure" }
                add_field => { "message_type" => "disco_detect_failed" }
                tag_on_failure => []
            }
        }

        if ![message_type] {
            grok {
                patterns_dir => ["/etc/logstash/patterns"]
                patterns_files_glob => "env-logs-patterns"
                match => { "message" => "^Successfully bound to TCP port \[port=%{NONNEGINT:[disco][srv][port]:int}, localHost=%{DATA:[disco][srv][addr]}, locNodeId=%{UUID:[node][id]}\]" }
                add_field => { "message_type" => "disco_srv_started" }
                tag_on_failure => []
            }
        }

        if ![message_type] {
            grok {
                patterns_dir => ["/etc/logstash/patterns"]
                patterns_files_glob => "env-logs-patterns"
                match => { "message" => "^TCP discovery accepted incoming connection \[rmtAddr=%{DATA:[disco][addr]}, rmtPort=%{NONNEGINT:[disco][port]:int}\]" }
                add_field => { "message_type" => "disco_srv_accepted" }
                tag_on_failure => []
            }
        }

        if ![message_type] {
            grok {
                patterns_dir => ["/etc/logstash/patterns"]
                patterns_files_glob => "env-logs-patterns"
                match => { "message" => "^TCP discovery spawning a new thread for connection \[rmtAddr=%{DATA:[disco][addr]}, rmtPort=%{NONNEGINT:[disco][port]:int}\]" }
                add_field => { "message_type" => "disco_conn_thread_started" }
                tag_on_failure => []
            }
        }

        if ![message_type] {
            grok {
                patterns_dir => ["/etc/logstash/patterns"]
                patterns_files_glob => "env-logs-patterns"
                match => { "message" => "^Started serving remote node connection \[rmtAddr=%{DATA:[disco][addr]}, rmtPort=%{NONNEGINT:[disco][port]:int}\]" }
                add_field => { "message_type" => "disco_conn_serving_started" }
                tag_on_failure => []
            }
        }

        if ![message_type] {
            grok {
                patterns_dir => ["/etc/logstash/patterns"]
                patterns_files_glob => "env-logs-patterns"
                match => { "message" => "^Started serving remote node connection \[rmtAddr=%{DATA:[disco][addr]}, rmtPort=%{NONNEGINT:[disco][port]:int}\]" }
                add_field => { "message_type" => "disco_conn_serving_started" }
                tag_on_failure => []
            }
        }

        if ![message_type] {
            grok {
                patterns_dir => ["/etc/logstash/patterns"]
                patterns_files_glob => "env-logs-patterns"
                match => { "message" => "^Finished serving remote node connection \[rmtAddr=%{DATA:[disco][addr]}, rmtPort=%{NONNEGINT:[disco][port]:int}" }
                add_field => { "message_type" => "disco_conn_serving_finished" }
                tag_on_failure => []
            }
        }

        if ![message_type] {
            grok {
                patterns_dir => ["/etc/logstash/patterns"]
                patterns_files_glob => "env-logs-patterns"
                match => { "message" => "^Node is out of topology" }
                add_field => { "message_type" => "disco_status_recon" }
                tag_on_failure => []
            }
        }

        if ![message_type] {
            grok {
                patterns_dir => ["/etc/logstash/patterns"]
                patterns_files_glob => "env-logs-patterns"
                match => { "message" => "^Communication SPI session write timed out.*?\[remoteAddr=%{DATA:[node][addr]}, writeTimeout=%{NONNEGINT:[tcp][write_timeout]:int}\]" }
                add_field => { "message_type" => "tcp_write_timeout" }
                tag_on_failure => []
            }
        }

        if ![message_type] {
            grok {
                patterns_dir => ["/etc/logstash/patterns"]
                patterns_files_glob => "env-logs-patterns"
                match => { "message" => "^Failed to connect to a remote node.*?\[addrs=\[%{DATA:[node][addr]}\]\]" }
                add_field => { "message_type" => "tcp_conn_failed" }
                tag_on_failure => []
            }
        }

        if ![message_type] {
            grok {
                patterns_dir => ["/etc/logstash/patterns"]
                patterns_files_glob => "env-logs-patterns"
                match => { "message" => "^Received message without registered handler.*?\[msg=%{WORD:[msg][type]}" }
                add_field => { "message_type" => "tcp_msg_no_handler" }
                tag_on_failure => []
            }
        }

        if ![message_type] {
            grok {
                patterns_dir => ["/etc/logstash/patterns"]
                patterns_files_glob => "env-logs-patterns"
                match => { "message" => "^Close incoming connection, unknown node \[nodeId=%{UUID:[node][id]}" }
                add_field => { "message_type" => "tcp_unknown_node" }
                tag_on_failure => []
            }
        }

        if ![message_type] {
            grok {
                patterns_dir => ["/etc/logstash/patterns"]
                patterns_files_glob => "env-logs-patterns"
                match => { "message" => "^Failed to read message" }
                add_field => { "message_type" => "tcp_msg_read_failed" }
                tag_on_failure => []
            }
        }

        if ![message_type] {
            grok {
                patterns_dir => ["/etc/logstash/patterns"]
                patterns_files_glob => "env-logs-patterns"
                match => { "message" => "^Failed to process selector key" }
                add_field => { "message_type" => "tcp_failed_close" }
                tag_on_failure => []
            }
        }

        if ![message_type] {
            grok {
                patterns_dir => ["/etc/logstash/patterns"]
                patterns_files_glob => "env-logs-patterns"
                match => { "message" => "^Connect timed out.*?\[addr=%{DATA:[node][addr]}, connTimeout=%{NONNEGINT:[tcp][conn_timeout]:int}\]" }
                add_field => { "message_type" => "tcp_conn_timeout" }
                tag_on_failure => []
            }
        }

        if ![message_type] {
            grok {
                patterns_dir => ["/etc/logstash/patterns"]
                patterns_files_glob => "env-logs-patterns"
                match => { "message" => "^Handshake timedout.*?\[node=%{UUID:[node][id]}, timeout=%{NONNEGINT:[tcp][conn_timeout]:int}, maxConnTimeout=%{NONNEGINT:[tcp][max_conn_timeout]:int}, attempt=%{NONNEGINT:[tcp][attempts]:int}, reconCnt=%{NONNEGINT:[tcp][max_attempts]:int}, err=%{DATA:error}, addr=%{DATA:[node][addr]}\]" }
                add_field => { "message_type" => "tcp_handshake_timeout" }
                tag_on_failure => []
            }
        }

        if ![message_type] {
            grok {
                patterns_dir => ["/etc/logstash/patterns"]
                patterns_files_glob => "env-logs-patterns"
                match => { "message" => "^Successfully bound communication NIO server to TCP port \[port=%{NONNEGINT:[tcp][srv][port]:int}, locHost=%{DATA:[tcp][srv][addr]}," }
                add_field => { "message_type" => "tcp_srv_started" }
                tag_on_failure => []
            }
        }

        if ![message_type] {
            grok {
                patterns_dir => ["/etc/logstash/patterns"]
                patterns_files_glob => "env-logs-patterns"
                match => { "message" => "^Accepted incoming communication connection \[locAddr=%{DATA:[tcp][loc][addr]}, rmtAddr=%{DATA:[tcp][rmt][addr]}\]" }
                add_field => { "message_type" => "tcp_srv_accepted" }
                tag_on_failure => []
            }
        }

        if ![message_type] {
            grok {
                patterns_dir => ["/etc/logstash/patterns"]
                patterns_files_glob => "env-logs-patterns"
                match => { "message" => "^Established outgoing communication connection \[locAddr=%{DATA:[tcp][loc][addr]}, rmtAddr=%{DATA:[tcp][rmt][addr]}\]" }
                add_field => { "message_type" => "tcp_conn_started" }
                tag_on_failure => []
            }
        }

        if ![message_type] {
            grok {
                patterns_dir => ["/etc/logstash/patterns"]
                patterns_files_glob => "env-logs-patterns"
                match => { "message" => "^Received incoming connection from remote node while connecting to this node, rejecting \[locNode=%{UUID:[loc][node][id]}, locNodeOrder=%{NONNEGINT:[loc][node][order]:int}, rmtNode=%{UUID:[rmt][node][id]}, rmtNodeOrder=%{NONNEGINT:[rmt][node][order]:int}\]" }
                match => { "message" => "^Received incoming connection when already connected to this node, rejecting \[locNode=%{UUID:[loc][node][id]}, rmtNode=%{UUID:[rmt][node][id]}\]" }
                add_field => { "message_type" => "tcp_cross_conn_rejected" }
                tag_on_failure => []
            }
        }

        if ![message_type] {
            grok {
                patterns_dir => ["/etc/logstash/patterns"]
                patterns_files_glob => "env-logs-patterns"
                match => { "message" => "^Failed processing message \[senderId=%{UUID:[node][id]}, msg=%{WORD:[msg][type]}" }
                match => { "message" => "^Failed to process message \[senderId=%{UUID:[node][id]}, messageType=%{DATA:[msg][type]}\]" }
                add_field => { "message_type" => "msg_failed" }
                tag_on_failure => []
            }
        }

        if ![message_type] {
            grok {
                patterns_dir => ["/etc/logstash/patterns"]
                patterns_files_glob => "env-logs-patterns"
                match => { "message" => "^Failed to send message" }
                match => { "message" => "^Failed to send error message" }
                add_field => { "message_type" => "msg_send_failed" }
                tag_on_failure => []
            }
        }

        if ![message_type] {
            grok {
                patterns_dir => ["/etc/logstash/patterns"]
                patterns_files_glob => "env-logs-patterns"
                match => { "message" => "^Failed to activate node components \[nodeId=%{UUID:[node][id]}, client=%{WORD:[node][client]}, topVer=%{TOP_VER}\]" }
                add_field => { "message_type" => "activate_failed" }
                tag_on_failure => []
            }
        }

        if ![message_type] {
            grok {
                patterns_dir => ["/etc/logstash/patterns"]
                patterns_files_glob => "env-logs-patterns"
                match => { "message" => "^Started state transition: %{WORD:[cluster][active]}" }
                add_field => { "message_type" => "cluster_state_change_started" }
                tag_on_failure => []
            }
        }

        if ![message_type] {
            grok {
                patterns_dir => ["/etc/logstash/patterns"]
                patterns_files_glob => "env-logs-patterns"
                match => { "message" => "^Received state change finish message: %{WORD:[cluster][active]}" }
                add_field => { "message_type" => "cluster_state_change_finished" }
                tag_on_failure => []
            }
        }

        if ![message_type] {
            grok {
                patterns_dir => ["/etc/logstash/patterns"]
                patterns_files_glob => "env-logs-patterns"
                match => { "message" => "^Start activation process \[nodeId=%{UUID:[node][id]}, client=%{WORD:[node][client]}, topVer=%{TOP_VER}\]" }
                add_field => { "message_type" => "cluster_activate_started" }
                tag_on_failure => []
            }
        }

        if ![message_type] {
            grok {
                patterns_dir => ["/etc/logstash/patterns"]
                patterns_files_glob => "env-logs-patterns"
                match => { "message" => "^Successfully performed final activation steps \[nodeId=%{UUID:[node][id]}, client=%{WORD:[node][client]}, topVer=%{TOP_VER}\]" }
                add_field => { "message_type" => "cluster_activate_finished" }
                tag_on_failure => []
            }
        }

        if ![message_type] {
            grok {
                patterns_dir => ["/etc/logstash/patterns"]
                patterns_files_glob => "env-logs-patterns"
                match => { "message" => "^Start deactivation process \[nodeId=%{UUID:[node][id]}, client=%{WORD:[node][client]}, topVer=%{TOP_VER}\]" }
                add_field => { "message_type" => "cluster_deactivate_started" }
                tag_on_failure => []
            }
        }

        if ![message_type] {
            grok {
                patterns_dir => ["/etc/logstash/patterns"]
                patterns_files_glob => "env-logs-patterns"
                match => { "message" => "^Successfully deactivated data structures, services and caches \[nodeId=%{UUID:[node][id]}, client=%{WORD:[node][client]}, topVer=%{TOP_VER}\]" }
                add_field => { "message_type" => "cluster_deactivate_finished" }
                tag_on_failure => []
            }
        }

        if ![message_type] {
            grok {
                patterns_dir => ["/etc/logstash/patterns"]
                patterns_files_glob => "env-logs-patterns"
                match => { "message" => "^Failed to wait for completion of partition map exchange" }
                add_field => { "message_type" => "exch_failed" }
                tag_on_failure => []
            }
        }

        if ![message_type] {
            grok {
                patterns_dir => ["/etc/logstash/patterns"]
                patterns_files_glob => "env-logs-patterns"
                match => { "message" => "^Started exchange init \[topVer=%{TOP_VER}, crd=%{WORD:[node][crd]}, evt=%{WORD:[evt][type]}, evtNode=%{UUID:[node][id], customEvt=%{WORD:[msg][type]}" }
                add_field => { "message_type" => "exch_init_started" }
                tag_on_failure => []
            }
        }

        if ![message_type] {
            grok {
                patterns_dir => ["/etc/logstash/patterns"]
                patterns_files_glob => "env-logs-patterns"
                match => { "message" => "^Finished exchange init \[topVer=%{TOP_VER}, crd=%{WORD:[node][crd]}\]" }
                add_field => { "message_type" => "exch_init_finished" }
                tag_on_failure => []
            }
        }

        if ![message_type] {
            grok {
                patterns_dir => ["/etc/logstash/patterns"]
                patterns_files_glob => "env-logs-patterns"
                match => { "message" => "^finishExchangeOnCoordinator \[topVer=%{TOP_VER:initVer}, resVer=%{TOP_VER:resVer}\]" }
                add_field => { "message_type" => "exch_finished" }
                tag_on_failure => []
            }
        }

        if ![message_type] {
            grok {
                patterns_dir => ["/etc/logstash/patterns"]
                patterns_files_glob => "env-logs-patterns"
                match => { "message" => "^Finish exchange future \[startVer=%{TOP_VER:startVer}, resVer=%{TOP_VER:resVer}," }
                add_field => { "message_type" => "exch_fut_finished" }
                tag_on_failure => []
            }
        }

        if ![message_type] {
            grok {
                patterns_dir => ["/etc/logstash/patterns"]
                patterns_files_glob => "env-logs-patterns"
                match => { "message" => "^Send restore state response \[node=%{UUID:[node][id]}, exchVer=%{TOP_VER}," }
                add_field => { "message_type" => "exch_send_restore_response" }
                tag_on_failure => []
            }
        }

        if ![message_type] {
            grok {
                patterns_dir => ["/etc/logstash/patterns"]
                patterns_files_glob => "env-logs-patterns"
                match => { "message" => "^Coordinator changed, send partitions to new coordinator \[ver=%{TOP_VER}, crd=%{UUID:[crd][node][id]}, newCrd=%{UUID:[new][crd][id]}\]" }
                match => { "message" => "^Coordinator failed, node is new coordinator \[ver=%{TOP_VER}, prev=%{UUID:[old][crd][id]}\]" }
                match => { "message" => "^Init new coordinator.*?\[node=%{UUID:[new][crd][id]}" }
                match => { "message" => "^Init new coordinator future will skip remote node: %{DISCO_NODE}" }
                match => { "message" => "^Init new coordinator future will skip remote node" }
                match => { "message" => "^Try restore exchange result" }
                match => { "message" => "^New coordinator restore state finished \[ver=%{TOP_VER}" }
                match => { "message" => "^Received partitions request, change coordinator \[oldCrd=%{UUID:[old][crd][id]}, newCrd=%{UUID:[new][crd][id]}\]" }
                add_field => { "message_type" => "exch_crd_changed" }
                tag_on_failure => []
            }

            if [message_type] == "exch_crd_changed" {
                mutate {
                    replace => { "level" => "WARN" }
                }
            }
        }

        if ![message_type] {
            grok {
                patterns_dir => ["/etc/logstash/patterns"]
                patterns_files_glob => "env-logs-patterns"
                match => { "message" => "^Waiting for coordinator initialization \[node=%{UUID:[node][id]}, nodeOrder=%{NONNEGINT:[node][order]:int}, ver=%{TOP_VER}\]" }
                add_field => { "message_type" => "exch_wait_crd_init" }
                tag_on_failure => []
            }
        }

        if ![message_type] {
            grok {
                patterns_dir => ["/etc/logstash/patterns"]
                patterns_files_glob => "env-logs-patterns"
                match => { "message" => "^Process pending message on coordinator \[node=%{UUID:[node][id]}, ver=%{TOP_VER:curVer}, resVer=%{TOP_VER:resVer}\]" }
                add_field => { "message_type" => "exch_crd_pending_msg" }
                tag_on_failure => []
            }
        }

        if ![message_type] {
            grok {
                patterns_dir => ["/etc/logstash/patterns"]
                patterns_files_glob => "env-logs-patterns"
                match => { "message" => "^Coordinator received single message \[ver=%{TOP_VER}, node=%{UUID:[node][id]}, allReceived=%{WORD:[msg][all]}" }
                match => { "message" => "^Received single message, already done \[ver=%{TOP_VER}, node=%{UUID:[node][id]}" }
                match => { "message" => "^Non-coordinator received single message" }
                add_field => { "message_type" => "exch_msg_received" }
                tag_on_failure => []
            }
        }

        if ![message_type] {
            grok {
                patterns_dir => ["/etc/logstash/patterns"]
                patterns_files_glob => "env-logs-patterns"
                match => { "message" => "^Coordinator received all messages, try merge \[ver=%{TOP_VER}\]" }
                add_field => { "message_type" => "exch_msg_all_received" }
                tag_on_failure => []
            }
        }

        if ![message_type] {
            grok {
                patterns_dir => ["/etc/logstash/patterns"]
                patterns_files_glob => "env-logs-patterns"
                match => { "message" => "^Merge server join exchange" }
                add_field => { "message_type" => "exch_merge_join" }
                tag_on_failure => []
            }
        }

        if ![message_type] {
            grok {
                patterns_dir => ["/etc/logstash/patterns"]
                patterns_files_glob => "env-logs-patterns"
                match => { "message" => "^Merge exchange future \[curFut=%{TOP_VER:curVer}, mergedFut=%{TOP_VER:mergedVer}, evt=%{WORD:[evt][type]}, evtNode=%{UUID:[node][id]}, evtNodeClient=%{WORD:[node][client]}\]" }
                match => { "message" => "^Merge exchange future on finish \[curFut=%{TOP_VER:curVer}, mergedFut=%{TOP_VER:mergedVer}, evt=%{WORD:[evt][type]}, evtNode=%{UUID:[node][id]}, evtNodeClient=%{WORD:[node][client]}\]" }
                match => { "message" => "^Merge exchange future on finish stop \[curFut=%{TOP_VER:curVer}, resVer=%{TOP_VER:resVer}, nextFutVer=%{TOP_VER:nextFutVer}\]" }
                add_field => { "message_type" => "exch_fut_merge" }
                tag_on_failure => []
            }
        }

        if ![message_type] {
            grok {
                patterns_dir => ["/etc/logstash/patterns"]
                patterns_files_glob => "env-logs-patterns"
                match => { "message" => "^Received full message, will finish exchange \[node=%{UUID:[node][id]}, resVer=%{TOP_VER:resVer}\]" }
                match => { "message" => "^Received full message, need merge \[curFut=%{TOP_VER:curVer}, resVer=%{TOP_VER:resVer}\]" }
                add_field => { "message_type" => "exch_full_msg_received" }
                tag_on_failure => []
            }
        }

        if ![message_type] {
            grok {
                patterns_dir => ["/etc/logstash/patterns"]
                patterns_files_glob => "env-logs-patterns"
                match => { "message" => "^Failed to send partitions full message \[node=%{DISCO_NODE}" }
                add_field => { "message_type" => "exch_full_msg_send_failed" }
                tag_on_failure => []
            }
        }

        if ![message_type] {
            grok {
                patterns_dir => ["/etc/logstash/patterns"]
                patterns_files_glob => "env-logs-patterns"
                match => { "message" => "^Ignore full message, future is done" }
                add_field => { "message_type" => "exch_full_msg_ignored" }
                tag_on_failure => []
            }
        }

        if ![message_type] {
            grok {
                patterns_dir => ["/etc/logstash/patterns"]
                patterns_files_glob => "env-logs-patterns"
                match => { "message" => "^Skipping rebalancing.*?\[top=%{TOP_VER}, evt=%{WORD:[evt][type]}, node=%{UUID:[node][id]}\]" }
                match => { "message" => "^Rebalancing skipped due to empty assignments" }
                match => { "message" => "^Rebalancing skipped due to cancelled assignments" }
                add_field => { "message_type" => "reb_skip" }
                tag_on_failure => []
            }
        }

        if ![message_type] {
            grok {
                patterns_dir => ["/etc/logstash/patterns"]
                patterns_files_glob => "env-logs-patterns"
                match => { "message" => "^Adding partition assignments" }
                add_field => { "message_type" => "reb_assignment" }
                tag_on_failure => []
            }
        }

        if ![message_type] {
            grok {
                patterns_dir => ["/etc/logstash/patterns"]
                patterns_files_glob => "env-logs-patterns"
                match => { "message" => "^Rebalancing scheduled" }
                add_field => { "message_type" => "reb_sched" }
                tag_on_failure => []
            }
        }

        if ![message_type] {
            grok {
                patterns_dir => ["/etc/logstash/patterns"]
                patterns_files_glob => "env-logs-patterns"
                match => { "message" => "^Cancelled rebalancing from all nodes \[topology=%{TOP_VER}" }
                add_field => { "message_type" => "reb_cancel" }
                tag_on_failure => []
            }
        }

        if ![message_type] {
            grok {
                patterns_dir => ["/etc/logstash/patterns"]
                patterns_files_glob => "env-logs-patterns"
                match => { "message" => "^Starting rebalancing \[mode=%{WORD:[reb][mode]}, fromNode=%{UUID:[node][id]}, partitionsCount=%{NONNEGINT:[part][count]:int}, topology=%{TOP_VER}, updateSeq=%{NONNEGINT:updateSeq:int}\]" }
                add_field => { "message_type" => "reb_starting" }
                tag_on_failure => []
            }
        }

        if ![message_type] {
            grok {
                patterns_dir => ["/etc/logstash/patterns"]
                patterns_files_glob => "env-logs-patterns"
                match => { "message" => "^Rebalancing started \[top=%{TOP_VER}, evt=%{WORD:[evt][type]}, node=%{UUID:[node][id]}\]" }
                add_field => { "message_type" => "reb_started" }
                tag_on_failure => []
            }
        }

        if ![message_type] {
            grok {
                patterns_dir => ["/etc/logstash/patterns"]
                patterns_files_glob => "env-logs-patterns"
                match => { "message" => "^Completed rebalancing \[fromNode=%{UUID:[node][id]}, topology=%{TOP_VER}, time=%{NONNEGINT:[reb][time]:int} ms\]" }
                add_field => { "message_type" => "reb_completed" }
                tag_on_failure => []
            }
        }

        if ![message_type] {
            grok {
                patterns_dir => ["/etc/logstash/patterns"]
                patterns_files_glob => "env-logs-patterns"
                match => { "message" => "^Completed \(final\) rebalancing \[fromNode=%{UUID:[node][id]}, topology=%{TOP_VER}, time=%{NONNEGINT:[reb][time]:int} ms\]" }
                add_field => { "message_type" => "reb_finished" }
                tag_on_failure => []
            }
        }

        if ![message_type] {
            grok {
                patterns_dir => ["/etc/logstash/patterns"]
                patterns_files_glob => "env-logs-patterns"
                match => { "message" => "^Requested rebalancing" }
                add_field => { "message_type" => "reb_requested" }
                tag_on_failure => []
            }
        }

        if ![message_type] {
            grok {
                patterns_dir => ["/etc/logstash/patterns"]
                patterns_files_glob => "env-logs-patterns"
                match => { "message" => "^Received supply message" }
                add_field => { "message_type" => "reb_supply_msg_received" }
                tag_on_failure => []
            }
        }

        if ![message_type] {
            grok {
                patterns_dir => ["/etc/logstash/patterns"]
                patterns_files_glob => "env-logs-patterns"
                match => { "message" => "^Finished rebalancing partition" }
                add_field => { "message_type" => "reb_part_finished" }
                tag_on_failure => []
            }
        }

        if ![message_type] {
            grok {
                patterns_dir => ["/etc/logstash/patterns"]
                patterns_files_glob => "env-logs-patterns"
                match => { "message" => "^Rebalancing key" }
                add_field => { "message_type" => "reb_key" }
                tag_on_failure => []
            }
        }

        if ![message_type] {
            grok {
                patterns_dir => ["/etc/logstash/patterns"]
                patterns_files_glob => "env-logs-patterns"
                match => { "message" => "^Rebalancing entry is already in cache" }
                add_field => { "message_type" => "reb_entry_already_cached" }
                tag_on_failure => []
            }
        }

        if ![message_type] {
            grok {
                patterns_dir => ["/etc/logstash/patterns"]
                patterns_files_glob => "env-logs-patterns"
                match => { "message" => "^Entry has been concurrently removed while rebalancing" }
                add_field => { "message_type" => "reb_entry_concur_removed" }
                tag_on_failure => []
            }
        }

        if ![message_type] {
            grok {
                patterns_dir => ["/etc/logstash/patterns"]
                patterns_files_glob => "env-logs-patterns"
                match => { "message" => "^Completed rebalance future" }
                add_field => { "message_type" => "reb_fut_completed" }
                tag_on_failure => []
            }
        }

        if ![message_type] {
            grok {
                patterns_dir => ["/etc/logstash/patterns"]
                patterns_files_glob => "env-logs-patterns"
                match => { "message" => "^Stale update for single partition map update" }
                add_field => { "message_type" => "top_stale_part_map" }
                tag_on_failure => []
            }
        }

        if ![message_type] {
            grok {
                patterns_dir => ["/etc/logstash/patterns"]
                patterns_files_glob => "env-logs-patterns"
                match => { "message" => "^Stale version for full partition map update message" }
                add_field => { "message_type" => "top_stale_full_part_map" }
                tag_on_failure => []
            }
        }

        if ![message_type] {
            grok {
                patterns_dir => ["/etc/logstash/patterns"]
                patterns_files_glob => "env-logs-patterns"
                match => { "message" => "^Requested topology version does not match calculated diff, will require full iteration tocalculate mapping" }
                add_field => { "message_type" => "top_calc_diff_not_match" }
                tag_on_failure => []
            }
        }

        if ![message_type] {
            grok {
                patterns_dir => ["/etc/logstash/patterns"]
                patterns_files_glob => "env-logs-patterns"
                match => { "message" => "^Added node failed message for node from failed nodes list: TcpDiscoveryNodeFailedMessage \[failedNodeId=%{UUID:[node][id]}," }
                add_field => { "message_type" => "top_node_failing" }
                tag_on_failure => []
            }
        }

        if ![message_type] {
            grok {
                patterns_dir => ["/etc/logstash/patterns"]
                patterns_files_glob => "env-logs-patterns"
                match => { "message" => "^Node FAILED: %{DISCO_NODE}" }
                match => { "message" => "^Node FAILED" }
                add_field => { "message_type" => "top_node_failed" }
                tag_on_failure => []
            }
        }

        if ![message_type] {
            grok {
                patterns_dir => ["/etc/logstash/patterns"]
                patterns_files_glob => "env-logs-patterns"
                match => { "message" => "^Local node SEGMENTED: %{DISCO_NODE}" }
                match => { "message" => "^Local node SEGMENTED" }
                add_field => { "message_type" => "top_node_segmented" }
                tag_on_failure => []
            }
        }

        if ![message_type] {
            grok {
                patterns_dir => ["/etc/logstash/patterns"]
                patterns_files_glob => "env-logs-patterns"
                match => { "message" => "^Added new node to topology: %{DISCO_NODE}" }
                add_field => { "message_type" => "top_node_added" }
                tag_on_failure => []
            }
        }

        if ![message_type] {
            grok {
                patterns_dir => ["/etc/logstash/patterns"]
                patterns_files_glob => "env-logs-patterns"
                match => { "message" => "^Node left topology: %{DISCO_NODE}" }
                add_field => { "message_type" => "top_node_left" }
                tag_on_failure => []
            }
        }

        if ![message_type] {
            grok {
                patterns_dir => ["/etc/logstash/patterns"]
                patterns_files_glob => "env-logs-patterns"
                match => { "message" => "^No verification for local node leave has been received from coordinator" }
                add_field => { "message_type" => "top_node_leave" }
                tag_on_failure => []
            }
        }

        if ![message_type] {
            grok {
                patterns_dir => ["/etc/logstash/patterns"]
                patterns_files_glob => "env-logs-patterns"
                match => { "message" => "^>>> ver\. %{DATA:[ignite][ver]#%{DATA:[ignite][date]}-sha1:%{DATA:[ignite][hash]}" }
                add_field => { "message_type" => "top_node_starting" }
                tag_on_failure => []
            }
        }

        if ![message_type] {
            grok {
                patterns_dir => ["/etc/logstash/patterns"]
                patterns_files_glob => "env-logs-patterns"
                match => { "message" => "^Stopping local node according to configured segmentation policy" }
                match => { "message" => "^Stopping local node" }
                add_field => { "message_type" => "top_node_stopping" }
                tag_on_failure => []
            }
        }

        if ![message_type] {
            grok {
                patterns_dir => ["/etc/logstash/patterns"]
                patterns_files_glob => "env-logs-patterns"
                match => { "message" => "^>>>\s*\+\-+\+" }
                add_field => { "message_type" => "top_node_start_stop" }
                tag_on_failure => []
            }

            if [message_type] {
                mutate {
                    remove_field => [ "message_type" ]
                }

                if ![message_type] {
                    grok {
                        patterns_dir => ["/etc/logstash/patterns"]
                        patterns_files_glob => "env-logs-patterns"
                        match => { "message" => " stopped with ERRORS" }
                        add_field => { "message_type" => "top_node_stopped_error" }
                        tag_on_failure => []
                    }
                }

                if ![message_type] {
                    grok {
                        patterns_dir => ["/etc/logstash/patterns"]
                        patterns_files_glob => "env-logs-patterns"
                        match => { "message" => " stopped OK" }
                        add_field => { "message_type" => "top_node_stopped_ok" }
                        tag_on_failure => []
                    }
                }

                if ![message_type] {
                    mutate {
                add_field => { "message_type" => "top_node_started" }
                    }
                }

                grok {
                    patterns_dir => ["/etc/logstash/patterns"]
                    patterns_files_glob => "env-logs-patterns"
                    match => { "message" => "Ignite ver. %{DATA:[ignite][ver]#%{DATA:[ignite][date]}-sha1:%{DATA:[ignite][hash]}" }
                    tag_on_failure => []
                }

                grok {
                    patterns_dir => ["/etc/logstash/patterns"]
                    patterns_files_glob => "env-logs-patterns"
                    match => { "message" => "OS name: %{DATA:[node][os][user]}" }
                    tag_on_failure => []
                }

                grok {
                    patterns_dir => ["/etc/logstash/patterns"]
                    patterns_files_glob => "env-logs-patterns"
                    match => { "message" => "CPU(s): %{NONNEGINT:[node][cpu][count]:int}" }
                    tag_on_failure => []
                }

                grok {
                    patterns_dir => ["/etc/logstash/patterns"]
                    patterns_files_glob => "env-logs-patterns"
                    match => { "message" => "Heap: %{DATA:[node][heap][size]}" }
                    tag_on_failure => []
                }

                grok {
                    patterns_dir => ["/etc/logstash/patterns"]
                    patterns_files_glob => "env-logs-patterns"
                    match => { "message" => "VM name: %{DATA:[node][jvm][name]}" }
                    tag_on_failure => []
                }

                grok {
                    patterns_dir => ["/etc/logstash/patterns"]
                    patterns_files_glob => "env-logs-patterns"
                    match => { "message" => "Ignite instance name: %{DATA:[node][name]}" }
                tag_on_failure => []
            }

                grok {
                    patterns_dir => ["/etc/logstash/patterns"]
                    patterns_files_glob => "env-logs-patterns"
                    match => { "message" => "Local node \[ID=%{UUID:[node][id]}, order=%{NONNEGINT:[node][order]:int}, clientMode=%{WORD:[node][client]}\]" }
                    tag_on_failure => []
                }

                grok {
                    patterns_dir => ["/etc/logstash/patterns"]
                    patterns_files_glob => "env-logs-patterns"
                    match => { "message" => "Local node addresses: \[%{DATA:[node][addrs]}\]" }
                    tag_on_failure => []
                }

                grok {
                    patterns_dir => ["/etc/logstash/patterns"]
                    patterns_files_glob => "env-logs-patterns"
                    match => { "message" => "Local ports: %{GREEDYDATA:[node][ports]}" }
                    tag_on_failure => []
                }
            }
        }

        if ![message_type] {
            grok {
                patterns_dir => ["/etc/logstash/patterns"]
                patterns_files_glob => "env-logs-patterns"
                match => { "message" => "^OS user: %{DATA:[node][os][user]}" }
                match => { "message" => "^OS: %{DATA:[node][os][name]}" }
                match => { "message" => "^Config URL: %{DATA:[node][cfg][url]}" }
                match => { "message" => "^Daemon mode: %{WORD:[node][daemon]}" }
                match => { "message" => "^PID: %{NONNEGINT:[node][pid]:int}" }
                match => { "message" => "^Language runtime: %{DATA:[node][jvm][lang]}" }
                match => { "message" => "^VM information: %{DATA:[node][jvm][ver]}" }
                match => { "message" => "^VM arguments: %{DATA:[node][jvm][args]}" }
                match => { "message" => "^VM total memory: %{DATA:[node][jvm][mem][total]}" }
                match => { "message" => "^IGNITE_HOME=%{DATA:[node][home_dir]}" }
                match => { "message" => "^Resolved directory for serialized binary metadata: %{DATA:[node][bin_meta_dir]}" }
                match => { "message" => "^Resolved write ahead log work directory: %{DATA:[node][wal_dir]}" }
                match => { "message" => "^Resolved page store work directory: %{DATA:[node][page_store_dir]}" }
                match => { "message" => "^Resolved write ahead log archive directory: %{DATA:[node][wal_archive_dir]}" }
                match => { "message" => "^Local node user attribute \[%{DATA:[node][attr]}\]" }
                match => { "message" => "^Remote Management %{DATA:[node][remote_mgmt]}" }
                match => { "message" => "^Configured caches %{DATA:[node][cfg][caches]}" }
                match => { "message" => "^System cache's MemoryPolicy size is configured to %{NONNEGINT:[node][mem][sys_cache_size]:int} MB" }
                match => { "message" => "^System cache's DataRegion size is configured to %{NONNEGINT:[node][mem][sys_cache_size]:int} MB" }
                match => { "message" => "^Wal history size is overridden" }
                match => { "message" => "^Security status " }
                match => { "message" => "^Non-loopback local IPs: " }
                match => { "message" => "^Enabled local MACs: " }
                match => { "message" => "^Checkpoints are disabled" }
                match => { "message" => "^Collision resolution is disabled" }
                match => { "message" => "^Failure detection timeout will be ignored" }
                match => { "message" => "^Message queue limit is set to 0" }
                match => { "message" => "^Peer class loading is enabled" }
                match => { "message" => "^Rolling updates are disabled" }
                match => { "message" => "^User or internal attribute has the same name as environment or system property and will take precedence" }
                match => { "message" => "^Data Regions Configured:" }
                match => { "message" => "^\s*\^-- %{DATA:[data_region][name]} \[initSize=%{DATA:[data_region][init_size]}, maxSize=%{DATA:[data_region][max_size]}, persistenceEnabled=%{WORD:[data_region][persistence]}\]" }
                match => { "message" => "^Performance suggestions for grid" }
                match => { "message" => "^Refer to this page for more performance suggestions: " }
                match => { "message" => "^To disable, set -DIGNITE_PERFORMANCE_SUGGESTIONS_DISABLED=true" }
                match => { "message" => "^\^-- Disable rolling update" }
                match => { "message" => "^\^-- Reduce pages swapping ratio" }
                match => { "message" => "^\^-- Set max direct memory size" }
                match => { "message" => "^\^-- Disable assertions" }
                match => { "message" => "^\^-- Disable rolling updates" }
                match => { "message" => "^\^-- Reduce pages swapping ratio" }
                match => { "message" => "^\^-- Decrease number of backups" }
                match => { "message" => "^\^-- Disable fully synchronous writes" }
                match => { "message" => "^To start Console Management & Monitoring run" }
                match => { "message" => "^3-rd party licenses can be found at" }
                match => { "message" => "^\^-- 2017 Copyright\(C\) GridGain Systems" }
                match => { "message" => "^Configured plugins:" }
                match => { "message" => "^\^-- GridGain %{DATA:[gg][ver]#%{DATA:[gg][date]}-sha1:%{DATA:[gg][hash]}" }
                add_field => { "message_type" => "node_start" }
                remove_field => [ "level" ]
                tag_on_failure => []
            }
        }

        if ![message_type] {
            grok {
                patterns_dir => ["/etc/logstash/patterns"]
                patterns_files_glob => "env-logs-patterns"
                match => { "message" => "^IgniteConfiguration" }
                add_field => { "message_type" => "node_config" }
                tag_on_failure => []
            }
        }

        if ![message_type] {
            grok {
                patterns_dir => ["/etc/logstash/patterns"]
                patterns_files_glob => "env-logs-patterns"
                match => { "message" => "BaselineTopology\w*\s*\[id=%{NONNEGINT:[baseline][id]}" }
                add_field => { "message_type" => "top_baseline" }
                tag_on_failure => []
            }
        }

        if ![message_type] {
            grok {
                patterns_dir => ["/etc/logstash/patterns"]
                patterns_files_glob => "env-logs-patterns"
                match => { "message" => "^Point in time recovery is enabled" }
                add_field => { "message_type" => "pitr_enabled" }
                tag_on_failure => []
            }
        }

        if ![message_type] {
            grok {
                patterns_dir => ["/etc/logstash/patterns"]
                patterns_files_glob => "env-logs-patterns"
                match => { "message" => "^Started cache \[name=%{DATA:[cache][name]}(, id=%{INT:[cache][id]:int})?(, group=%{DATA:[cache][group]})?, memoryPolicyName=%{DATA:[mem][policy][name]}, mode=%{WORD:[cache][mode]}, atomicity=%{WORD:[cache][atomicity]}(, backups=%{INT:[cache][backups]:int})?\]" }
                match => { "message" => "^Started cache" }
                add_field => { "message_type" => "cache_started" }
                tag_on_failure => []
            }
        }

        if ![message_type] {
            grok {
                patterns_dir => ["/etc/logstash/patterns"]
                patterns_files_glob => "env-logs-patterns"
                match => { "message" => "^Stopped cache \[cacheName=%{DATA:[cache][name]}, group=%{DATA:[cache][group]}\]" }
                match => { "message" => "^Stopped cache" }
                add_field => { "message_type" => "cache_stopped" }
                tag_on_failure => []
            }
        }

        if ![message_type] {
            grok {
                patterns_dir => ["/etc/logstash/patterns"]
                patterns_files_glob => "env-logs-patterns"
                match => { "message" => "^Received metadata on join" }
                add_field => { "message_type" => "type_meta_received" }
                tag_on_failure => []
            }
        }

        if ![message_type] {
            grok {
                patterns_dir => ["/etc/logstash/patterns"]
                patterns_files_glob => "env-logs-patterns"
                match => { "message" => "^Requesting metadata update" }
                add_field => { "message_type" => "type_meta_update_requesting" }
                tag_on_failure => []
            }
        }

        if ![message_type] {
            grok {
                patterns_dir => ["/etc/logstash/patterns"]
                patterns_files_glob => "env-logs-patterns"
                match => { "message" => "^Waiting for update for \[typeId" }
                add_field => { "message_type" => "type_meta_update_waiting" }
                tag_on_failure => []
            }
        }

        if ![message_type] {
            grok {
                patterns_dir => ["/etc/logstash/patterns"]
                patterns_files_glob => "env-logs-patterns"
                match => { "message" => "^Updated metadata on server node" }
                match => { "message" => "^Updated metadata on originating node" }
                add_field => { "message_type" => "type_meta_updated" }
                tag_on_failure => []
            }
        }

        if ![message_type] {
            grok {
                patterns_dir => ["/etc/logstash/patterns"]
                patterns_files_glob => "env-logs-patterns"
                match => { "message" => "^Completing future for \[typeId" }
                add_field => { "message_type" => "type_meta_fut_completing" }
                tag_on_failure => []
            }
        }

        if ![message_type] {
            grok {
                patterns_dir => ["/etc/logstash/patterns"]
                patterns_files_glob => "env-logs-patterns"
                match => { "message" => "^Versions are stamped on coordinator" }
                add_field => { "message_type" => "type_meta_coord_ver_stamped" }
                tag_on_failure => []
            }
        }

        if ![message_type] {
            grok {
                patterns_dir => ["/etc/logstash/patterns"]
                patterns_files_glob => "env-logs-patterns"
                match => { "message" => "^Successfully activated caches \[nodeId=%{UUID:[node][id]}, client=%{WORD:[node][client]}, topVer=%{TOP_VER}\]" }
                add_field => { "message_type" => "caches_activated" }
                tag_on_failure => []
            }
        }

        if ![message_type] {
            grok {
                patterns_dir => ["/etc/logstash/patterns"]
                patterns_files_glob => "env-logs-patterns"
                match => { "message" => "^Deleting all caches from affected cache groups" }
                add_field => { "message_type" => "caches_deleting" }
                tag_on_failure => []
            }
        }

        if ![message_type] {
            grok {
                patterns_dir => ["/etc/logstash/patterns"]
                patterns_files_glob => "env-logs-patterns"
                match => { "message" => "^Caches cleaning utility is started from node: \[%{UUID:[node][id]}," }
                add_field => { "message_type" => "cache_clean" }
                tag_on_failure => []
            }
        }

        if ![message_type] {
            grok {
                patterns_dir => ["/etc/logstash/patterns"]
                patterns_files_glob => "env-logs-patterns"
                match => { "message" => "^Finished indexes rebuilding for cache\: \[name=%{DATA:[cache][name]}, grpName=%{DATA:[cache][group]" }
                match => { "message" => "^Finished indexes rebuilding" }
                add_field => { "message_type" => "idx_rebuild_finished" }
                tag_on_failure => []
            }
        }

        if ![message_type] {
            grok {
                patterns_dir => ["/etc/logstash/patterns"]
                patterns_files_glob => "env-logs-patterns"
                match => { "message" => "^Failed to send event notification to node: %{UUID:[node][id]}" }
                add_field => { "message_type" => "evt_send_failed" }
                tag_on_failure => []
            }
        }

        if ![message_type] {
            grok {
                patterns_dir => ["/etc/logstash/patterns"]
                patterns_files_glob => "env-logs-patterns"
                match => { "message" => "^Failed to ping node \(status check will be initiated\): %{UUID:[node][id]}" }
                add_field => { "message_type" => "ping_failed" }
                tag_on_failure => []
            }
        }

        if ![message_type] {
            grok {
                patterns_dir => ["/etc/logstash/patterns"]
                patterns_files_glob => "env-logs-patterns"
                match => { "message" => "^Received ping request from the remote node \[rmtNodeId=%{UUID:[node][id]}, rmtAddr=%{DATA:[node][addr]}, rmtPort=%{NONNEGINT:[node][port]:int}\]" }
                add_field => { "message_type" => "ping_request" }
                tag_on_failure => []
            }
        }

        if ![message_type] {
            grok {
                patterns_dir => ["/etc/logstash/patterns"]
                patterns_files_glob => "env-logs-patterns"
                match => { "message" => "^Finished writing ping response \[rmtNodeId=%{UUID:[node][id]}, rmtAddr=%{DATA:[node][addr]}, rmtPort=%{NONNEGINT:[node][port]:int}\]" }
                add_field => { "message_type" => "ping_response" }
                tag_on_failure => []
            }
        }

        if ![message_type] {
            grok {
                patterns_dir => ["/etc/logstash/patterns"]
                patterns_files_glob => "env-logs-patterns"
                match => { "message" => "^Pinging node: %{UUID:[node][id]}" }
                add_field => { "message_type" => "ping_started" }
                tag_on_failure => []
            }
        }

        if ![message_type] {
            grok {
                patterns_dir => ["/etc/logstash/patterns"]
                patterns_files_glob => "env-logs-patterns"
                match => { "message" => "^Finished node ping \[nodeId=%{UUID:[node][id]}, res=%{WORD:[ping][result]}, time=%{NONNEGINT:[ping][time]:int}ms\]" }
                add_field => { "message_type" => "ping_finished" }
                tag_on_failure => []
            }
        }

        if ![message_type] {
            grok {
                patterns_dir => ["/etc/logstash/patterns"]
                patterns_files_glob => "env-logs-patterns"
                match => { "message" => "^Update status is not available" }
                add_field => { "message_type" => "upd_status_not_available" }
                tag_on_failure => []
            }
        }

        if ![message_type] {
            grok {
                patterns_dir => ["/etc/logstash/patterns"]
                patterns_files_glob => "env-logs-patterns"
                match => { "message" => "^Nodes started on local machine require more than.*?\[required=%{NONNEGINT:[mem][required]:int}MB, available=%{NONNEGINT:[mem][available]:int}MB\]" }
                add_field => { "message_type" => "mem_required" }
                remove_field => [ "level" ] # will be set to CONFIG
                tag_on_failure => []
            }
        }

        if ![message_type] {
            grok {
                patterns_dir => ["/etc/logstash/patterns"]
                patterns_files_glob => "env-logs-patterns"
                match => { "message" => "^Failed to wait for partition map exchange \[topVer=%{TOP_VER}, node=%{UUID:[node][id]}\]" }
                add_field => { "message_type" => "exch_wait" }
                tag_on_failure => []
            }
        }

        if ![message_type] {
            grok {
                patterns_dir => ["/etc/logstash/patterns"]
                patterns_files_glob => "env-logs-patterns"
                match => { "message" => "^Still waiting for initial partition map exchange.*?\[evtNode=%{DISCO_NODE}, topVer=%{INT:[top][ver]:int},.*?type=%{WORD:[evt][type]}, tstamp=" }
                add_field => { "message_type" => "exch_wait" }
                tag_on_failure => []
            }
        }

        if ![message_type] {
            grok {
                patterns_dir => ["/etc/logstash/patterns"]
                patterns_files_glob => "env-logs-patterns"
                match => { "message" => "^Failed to wait for initial partition map exchange" }
                add_field => { "message_type" => "exch_wait" }
                tag_on_failure => []
            }
        }

        if ![message_type] {
            grok {
                patterns_dir => ["/etc/logstash/patterns"]
                patterns_files_glob => "env-logs-patterns"
                match => { "message" => "^Failed to wait for partition release future \[topVer=%{TOP_VER}, node=%{UUID:[node][id]}\]" }
                add_field => { "message_type" => "exch_wait" }
                tag_on_failure => []
            }
        }

        if ![message_type] {
            grok {
                patterns_dir => ["/etc/logstash/patterns"]
                patterns_files_glob => "env-logs-patterns"
                match => { "message" => "^Ready affinity version: %{TOP_VER}" }
                add_field => { "message_type" => "exch_debug" }
                tag_on_failure => []
            }
        }

        if ![message_type] {
            grok {
                patterns_dir => ["/etc/logstash/patterns"]
                patterns_files_glob => "env-logs-patterns"
                match => { "message" => "^(First|Last) exchange future: " }
                add_field => { "message_type" => "exch_debug" }
                tag_on_failure => []
            }
        }

        if ![message_type] {
            grok {
                patterns_dir => ["/etc/logstash/patterns"]
                patterns_files_glob => "env-logs-patterns"
                match => { "message" => "^First %{NONNEGINT:[pending][affinity][shown]} pending affinity ready futures \[total=%{NONNEGINT:[pending][affinity][total]}\]" }
                add_field => { "message_type" => "exch_debug" }
                tag_on_failure => []
            }
        }

        if ![message_type] {
            grok {
                patterns_dir => ["/etc/logstash/patterns"]
                patterns_files_glob => "env-logs-patterns"
                match => { "message" => "^First %{NONNEGINT:[pending][affinity][shown]} pending affinity ready futures \[grp=%{DATA:[cache][group]}, total=%{NONNEGINT:[pending][affinity][total]}, lastVer=%{TOP_VER}\]" }
                add_field => { "message_type" => "exch_debug" }
                tag_on_failure => []
            }
        }

        if ![message_type] {
            grok {
                patterns_dir => ["/etc/logstash/patterns"]
                patterns_files_glob => "env-logs-patterns"
                match => { "message" => "^First %{NONNEGINT:[pending][exchange][shown]} pending exchange futures \[total=%{NONNEGINT:[pending][exchange][total]}\]" }
                add_field => { "message_type" => "exch_debug" }
                tag_on_failure => []
            }
        }

        if ![message_type] {
            grok {
                patterns_dir => ["/etc/logstash/patterns"]
                patterns_files_glob => "env-logs-patterns"
                match => { "message" => "^Last %{NONNEGINT:[exchange][shown]} exchange futures \(total: %{NONNEGINT:[exchange][total]}\):" }
                add_field => { "message_type" => "exch_debug" }
                tag_on_failure => []
            }
        }

        if ![message_type] {
            grok {
                patterns_dir => ["/etc/logstash/patterns"]
                patterns_files_glob => "env-logs-patterns"
                match => { "message" => "^Partition release future: PartitionReleaseFuture \[topVer=%{TOP_VER}" }
                add_field => { "message_type" => "exch_debug" }
                tag_on_failure => []
            }
        }

        if ![message_type] {
            grok {
                patterns_dir => ["/etc/logstash/patterns"]
                patterns_files_glob => "env-logs-patterns"
                match => { "message" => "^Failed to send diagnostic message" }
                add_field => { "message_type" => "exch_debug" }
                tag_on_failure => []
            }
        }

        if ![message_type] {
            grok {
                patterns_dir => ["/etc/logstash/patterns"]
                patterns_files_glob => "env-logs-patterns"
                match => { "message" => "^Still waiting for partition eviction" }
                add_field => { "message_type" => "exch_debug" }
                tag_on_failure => []
            }
        }

        if ![message_type] {
            grok {
                patterns_dir => ["/etc/logstash/patterns"]
                patterns_files_glob => "env-logs-patterns"
                match => { "message" => "^Partition has been scheduled for rebalancing due to outdated update counter \[nodeId=%{UUID:[node][id]}, (cacheOrGroupName|grp)?=%{DATA:[cache][name]}, partId=%{NONNEGINT:[part][id]:int}, haveHistory=%{WORD:[part][history]}\]" }
                match => { "message" => "^Partition has been scheduled for rebalancing due to outdated update counter" }
                add_field => { "message_type" => "exch_debug" }
                tag_on_failure => []
            }
        }

        if ![message_type] {
            grok {
                patterns_dir => ["/etc/logstash/patterns"]
                patterns_files_glob => "env-logs-patterns"
                match => { "message" => "^Finished waiting for partition release future \[topVer=%{TOP_VER}, waitTime=%{NONNEGINT:[part][wait_time]:int}ms," }
                add_field => { "message_type" => "exch_debug" }
                tag_on_failure => []
            }
        }

        if ![message_type] {
            grok {
                patterns_dir => ["/etc/logstash/patterns"]
                patterns_files_glob => "env-logs-patterns"
                match => { "message" => "^Pending cache messages waiting for exchange \[readyVer=%{TOP_VER:readyVer}, discoVer=%{INT:[disco][ver]:int}\]" }
                add_field => { "message_type" => "exch_debug" }
                tag_on_failure => []
            }
        }

        if ![message_type] {
            grok {
                patterns_dir => ["/etc/logstash/patterns"]
                patterns_files_glob => "env-logs-patterns"
                match => { "message" => "^Exchange future on coordinator waiting for server response \[node=%{UUID:[node][id]}, topVer=%{TOP_VER}\]" }
                add_field => { "message_type" => "exch_debug" }
                tag_on_failure => []
            }
        }

        if ![message_type] {
            grok {
                patterns_dir => ["/etc/logstash/patterns"]
                patterns_files_glob => "env-logs-patterns"
                match => { "message" => "^Exchange future waiting for coordinator response \[crd=%{UUID:[node][id]}, topVer=%{TOP_VER}\]" }
                add_field => { "message_type" => "exch_debug" }
                tag_on_failure => []
            }
        }

        if ![message_type] {
            grok {
                patterns_dir => ["/etc/logstash/patterns"]
                patterns_files_glob => "env-logs-patterns"
                match => { "message" => "^Pending transactions:" }
                match => { "message" => "^Pending explicit locks:" }
                match => { "message" => "^Pending cache futures:" }
                match => { "message" => "^Pending atomic cache futures:" }
                match => { "message" => "^Pending data streamer futures:" }
                match => { "message" => "^Pending transaction deadlock detection futures:" }
                match => { "message" => "^Dumping pending objects that might be the cause:" }
                match => { "message" => "^Rebalancing supplier reserved following partitions:" }
                add_field => { "message_type" => "exch_debug" }
                tag_on_failure => []
            }
        }

        if ![message_type] {
            grok {
                patterns_dir => ["/etc/logstash/patterns"]
                patterns_files_glob => "env-logs-patterns"
                match => { "message" => "^>>>\s+\w+\s+\[" }
                add_field => { "message_type" => "exch_debug" }
                tag_on_failure => []
            }
        }

        if ![message_type] {
            grok {
                patterns_dir => ["/etc/logstash/patterns"]
                patterns_files_glob => "env-logs-patterns"
                match => { "message" => "^>>>\s+\[txVer=" }
                add_field => { "message_type" => "exch_debug" }
                tag_on_failure => []
            }
        }

        if [message_type] in [ "exch_wait", "exch_debug"] {

            ruby {
                init => "require 'securerandom'"
                code => "event.set('level', 'DEBUG')"
            }

            aggregate {
                task_id => "%{source}"
                inactivity_timeout => 600
                code => "id = nil; ts = event.timestamp.time.to_f; map.each { |k, v| if (ts > (v - 5.0)) and (ts < (v + 5.0)) then id = k; break; end; }; if id == nil then id = SecureRandom.uuid; map[id] = ts; end; event.set('aggregate_id', id)"
            }
        }

        if [message_type] == "exch_wait" {

            aggregate {
                task_id => "%{aggregate_id}"
                inactivity_timeout => 10
                push_map_as_event_on_timeout => true
                code => "map.merge!(event.to_hash)"
                timeout_code => "if event.include?('message_type') then event.set('level', 'WARN'); event.set('message_type', 'exch_aggr'); m = event.get('message'); dm = {}; event.to_hash.each { |k, v| if k.start_with?('debug_message_') then off = k['debug_message_'.length..-1].to_i; dm[off] = v; end }; dm.sort.map { |off, msg| m += 10.chr + 10.chr + msg; event.remove('debug_message_' + off.to_s) }; event.set('message', m); else event.cancel(); end"
            }
        }

        if [message_type] == "exch_debug" {

            aggregate {
                task_id => "%{aggregate_id}"
                code => "m = event.get('message'); if m.length > 10000 then m = (m[0..4999] + '... ' + (m.length - 10000).to_s + ' char(s) ommited ...' + m[-5000..-1]) end; map['debug_message_' + event.get('offset').to_s] = m"
            }
        }

        if ![message_type] {
            grok {
                patterns_dir => ["/etc/logstash/patterns"]
                patterns_files_glob => "env-logs-patterns"
                match => { "message" => "^SLF4J:" }
                match => { "message" => "^Failed to resolve default logging config file" }
                add_field => { "message_type" => "logger" }
                tag_on_failure => []
            }

            if [message_type] == "logger" {
                mutate {
                    replace => { "level" => "CONFIG" }
                }
            }
        }

        if ![message_type] {
            grok {
                patterns_dir => ["/etc/logstash/patterns"]
                patterns_files_glob => "env-logs-patterns"
                match => { "message" => "^\s+at\s\w" }
                match => { "message" => "^Caused by:\s\w" }
                add_field => { "message_type" => "stack_trace" }
                tag_on_failure => []
            }
        }

        # Fix log message type
        if ![message_type] {
        grok {
            patterns_dir => ["/etc/logstash/patterns"]
            patterns_files_glob => "env-logs-patterns"
                match => { "message" => "^Fail" }
                match => { "message" => "[Ee]xception\b" }
                add_field => { "message_type" => "unknown_failure" }
            tag_on_failure => []
        }
        }
        if ![message_type] {
            mutate {
                add_field => { "message_type" => "unknown_log" }
            }
        }

        # Fix log level
        if [message_type] =~ /fail|error/ and [level] == "INFO" {
            mutate {
                replace => { "level" => "ERROR" }
            }
        }
        if ![level] {
            mutate {
                add_field => { "level" => "CONFIG" }
            }
        }
    }
    
    # Parsing GC log specific messages
    if [message_kind] == "gc" {
    
        if ![message_type] {
            grok {
                patterns_dir => ["/etc/logstash/patterns"]
                patterns_files_glob => "env-logs-patterns"
                match => { "message" => "^Total time for which application threads were stopped: %{BASE10NUM:[app][pause][total_time]:float} seconds, Stopping threads took: %{BASE10NUM:[app][pause][threads_time]:float} seconds" }
                add_field => { "message_type" => "app_pause" }
                tag_on_failure => []
            }
        }
        if ![message_type] {
            grok {
                patterns_dir => ["/etc/logstash/patterns"]
                patterns_files_glob => "env-logs-patterns"
                match => { "message" => "^\[%{DATA:[gc][msg][type]}\s*([,\]\d]|\(%{DATA:[gc][pause][type]}\))" }
                add_field => { "message_type" => "gc_pause" }
                tag_on_failure => []
            }
            
            if [message_type] == "gc_pause" {

                grok {
                    patterns_dir => ["/etc/logstash/patterns"]
                    patterns_files_glob => "env-logs-patterns"
                    match => { "message" => "\[Eden: %{BASE10NUM:[gc][eden][used][1]:int}%{SIZE_M:[gc][eden][used_m][1]}\(%{BASE10NUM:[gc][eden][comm][1]:int}%{SIZE_M:[gc][eden][comm_m][1]}\)->%{BASE10NUM:[gc][eden][used][2]:int}%{SIZE_M:[gc][eden][used_m][2]}\(%{BASE10NUM:[gc][eden][comm][2]:int}%{SIZE_M:[gc][eden][comm_m][2]}\) Survivors: %{BASE10NUM:[gc][surv][1]:int}%{SIZE_M:[gc][surv_m][1]}->%{BASE10NUM:[gc][surv][2]:int}%{SIZE_M:[gc][surv_m][2]} Heap: %{BASE10NUM:[gc][heap][used][1]:int}%{SIZE_M:[gc][heap][used_m][1]}\(%{BASE10NUM:[gc][heap][comm][1]:int}%{SIZE_M:[gc][heap][comm_m][1]}\)->%{BASE10NUM:[gc][heap][used][2]:int}%{SIZE_M:[gc][heap][used_m][2]}\(%{BASE10NUM:[gc][heap][comm][2]:int}%{SIZE_M:[gc][heap][comm_m][2]}\)\]" }
                    tag_on_failure => []
                }

                grok {
                    patterns_dir => ["/etc/logstash/patterns"]
                    patterns_files_glob => "env-logs-patterns"
                    match => { "message" => "\[Metaspace: %{BASE10NUM:[gc][meta][used][1]:int}%{SIZE_M:[gc][meta][used_m][1]}->%{BASE10NUM:[gc][meta][used][2]:int}%{SIZE_M:[gc][meta][used_m][2]}\(%{BASE10NUM:[gc][meta][comm]:int}%{SIZE_M:[gc][meta][comm_m]}\)\]" }
                    tag_on_failure => []
                }

                grok {
                    patterns_dir => ["/etc/logstash/patterns"]
                    patterns_files_glob => "env-logs-patterns"
                    match => { "message" => "\[Times: user=%{BASE10NUM:[gc][pause][time][user]:float} sys=%{BASE10NUM:[gc][pause][time][sys]:float}, real=%{BASE10NUM:[gc][pause][time][real]:float} secs\]" }
                    add_field => { "[app][pause][total_time]" => "%{[gc][pause][time][real]}" }
                    tag_on_failure => []
                }

                grok {
                    patterns_dir => ["/etc/logstash/patterns"]
                    patterns_files_glob => "env-logs-patterns"
                    match => { "message" => "^\s*%{BASE10NUM:[gc][heap][used][1]:int}%{SIZE_M:[gc][heap][used_m][1]}->%{BASE10NUM:[gc][heap][used][2]:int}%{SIZE_M:[gc][heap][used_m][2]}\(%{BASE10NUM:[gc][heap][comm][2]:int}%{SIZE_M:[gc][heap][comm_m][2]}\), %{BASE10NUM:[gc][pause][time][real]:float} secs\]" }
                    add_field => { "[app][pause][total_time]" => "%{[gc][pause][time][real]}" }
                    tag_on_failure => []
                }
            
                # Eden metrics
            
                if [gc][eden][used_m][1] == "B" {
                    mutate {
                        remove_field => [ "[gc][eden][used_m][1]" ]
                    }
                }
                if [gc][eden][used_m][1] == "K" {
                    ruby {
                        code => "event.set('[gc][eden][used][1]', event.get('[gc][eden][used][1]') * 1024)"
                        remove_field => [ "[gc][eden][used_m][1]" ]
                    }
                }
                if [gc][eden][used_m][1] == "M" {
                    ruby {
                        code => "event.set('[gc][eden][used][1]', event.get('[gc][eden][used][1]') * 1024 * 1024)"
                        remove_field => [ "[gc][eden][used_m][1]" ]
                    }
                }
                if [gc][eden][used_m][1] == "G" {
                    ruby {
                        code => "event.set('[gc][eden][used][1]', event.get('[gc][eden][used][1]') * 1024 * 1024 * 1024)"
                        remove_field => [ "[gc][eden][used_m][1]" ]
                    }
                }
                
                if [gc][eden][used_m][2] == "B" {
                    mutate {
                        remove_field => [ "[gc][eden][used_m][2]" ]
                    }
                }
                if [gc][eden][used_m][2] == "K" {
                    ruby {
                        code => "event.set('[gc][eden][used][2]', event.get('[gc][eden][used][2]') * 1024)"
                        remove_field => [ "[gc][eden][used_m][2]" ]
                    }
                }
                if [gc][eden][used_m][2] == "M" {
                    ruby {
                        code => "event.set('[gc][eden][used][2]', event.get('[gc][eden][used][2]') * 1024 * 1024)"
                        remove_field => [ "[gc][eden][used_m][2]" ]
                    }
                }
                if [gc][eden][used_m][2] == "G" {
                    ruby {
                        code => "event.set('[gc][eden][used][2]', event.get('[gc][eden][used][2]') * 1024 * 1024 * 1024)"
                        remove_field => [ "[gc][eden][used_m][2]" ]
                    }
                }
                
                if [gc][eden][comm_m][1] == "B" {
                    mutate {
                        remove_field => [ "[gc][eden][comm_m][1]" ]
                    }
                }
                if [gc][eden][comm_m][1] == "K" {
                    ruby {
                        code => "event.set('[gc][eden][comm][1]', event.get('[gc][eden][comm][1]') * 1024)"
                        remove_field => [ "[gc][eden][comm_m][1]" ]
                    }
                }
                if [gc][eden][comm_m][1] == "M" {
                    ruby {
                        code => "event.set('[gc][eden][comm][1]', event.get('[gc][eden][comm][1]') * 1024 * 1024)"
                        remove_field => [ "[gc][eden][comm_m][1]" ]
                    }
                }
                if [gc][eden][comm_m][1] == "G" {
                    ruby {
                        code => "event.set('[gc][eden][comm][1]', event.get('[gc][eden][comm][1]') * 1024 * 1024 * 1024)"
                        remove_field => [ "[gc][eden][comm_m][1]" ]
                    }
                }
                
                if [gc][eden][comm_m][2] == "B" {
                    mutate {
                        remove_field => [ "[gc][eden][comm_m][2]" ]
                    }
                }
                if [gc][eden][comm_m][2] == "K" {
                    ruby {
                        code => "event.set('[gc][eden][comm][2]', event.get('[gc][eden][comm][2]') * 1024)"
                        remove_field => [ "[gc][eden][comm_m][2]" ]
                    }
                }
                if [gc][eden][comm_m][2] == "M" {
                    ruby {
                        code => "event.set('[gc][eden][comm][2]', event.get('[gc][eden][comm][2]') * 1024 * 1024)"
                        remove_field => [ "[gc][eden][comm_m][2]" ]
                    }
                }
                if [gc][eden][comm_m][2] == "G" {
                    ruby {
                        code => "event.set('[gc][eden][comm][2]', event.get('[gc][eden][comm][2]') * 1024 * 1024 * 1024)"
                        remove_field => [ "[gc][eden][comm_m][2]" ]
                    }
                }
                
                # Survivor metrics
                
                if [gc][surv_m][1] == "B" {
                    mutate {
                        remove_field => [ "[gc][surv_m][1]" ]
                    }
                }
                if [gc][surv_m][1] == "K" {
                    ruby {
                        code => "event.set('[gc][surv][1]', event.get('[gc][surv][1]') * 1024)"
                        remove_field => [ "[gc][surv_m][1]" ]
                    }
                }
                if [gc][surv_m][1] == "M" {
                    ruby {
                        code => "event.set('[gc][surv][1]', event.get('[gc][surv][1]') * 1024 * 1024)"
                        remove_field => [ "[gc][surv_m][1]" ]
                    }
                }
                if [gc][surv_m][1] == "G" {
                    ruby {
                        code => "event.set('[gc][surv][1]', event.get('[gc][surv][1]') * 1024 * 1024 * 1024)"
                        remove_field => [ "[gc][surv_m][1]" ]
                    }
                }
                
                if [gc][surv_m][2] == "B" {
                    mutate {
                        remove_field => [ "[gc][surv_m][2]" ]
                    }
                }
                if [gc][surv_m][2] == "K" {
                    ruby {
                        code => "event.set('[gc][surv][2]', event.get('[gc][surv][2]') * 1024)"
                        remove_field => [ "[gc][surv_m][2]" ]
                    }
                }
                if [gc][surv_m][2] == "M" {
                    ruby {
                        code => "event.set('[gc][surv][2]', event.get('[gc][surv][2]') * 1024 * 1024)"
                        remove_field => [ "[gc][surv_m][2]" ]
                    }
                }
                if [gc][surv_m][2] == "G" {
                    ruby {
                        code => "event.set('[gc][surv][2]', event.get('[gc][surv][2]') * 1024 * 1024 * 1024)"
                        remove_field => [ "[gc][surv_m][2]" ]
                    }
                }
                
                # Heap metrics
                
                if [gc][heap][used_m][1] == "B" {
                    mutate {
                        remove_field => [ "[gc][heap][used_m][1]" ]
                    }
                }
                if [gc][heap][used_m][1] == "K" {
                    ruby {
                        code => "event.set('[gc][heap][used][1]', event.get('[gc][heap][used][1]') * 1024)"
                        remove_field => [ "[gc][heap][used_m][1]" ]
                    }
                }
                if [gc][heap][used_m][1] == "M" {
                    ruby {
                        code => "event.set('[gc][heap][used][1]', event.get('[gc][heap][used][1]') * 1024 * 1024)"
                        remove_field => [ "[gc][heap][used_m][1]" ]
                    }
                }
                if [gc][heap][used_m][1] == "G" {
                    ruby {
                        code => "event.set('[gc][heap][used][1]', event.get('[gc][heap][used][1]') * 1024 * 1024 * 1024)"
                        remove_field => [ "[gc][heap][used_m][1]" ]
                    }
                }
                
                if [gc][heap][used_m][2] == "B" {
                    mutate {
                        remove_field => [ "[gc][heap][used_m][2]" ]
                    }
                }
                if [gc][heap][used_m][2] == "K" {
                    ruby {
                        code => "event.set('[gc][heap][used][2]', event.get('[gc][heap][used][2]') * 1024)"
                        remove_field => [ "[gc][heap][used_m][2]" ]
                    }
                }
                if [gc][heap][used_m][2] == "M" {
                    ruby {
                        code => "event.set('[gc][heap][used][2]', event.get('[gc][heap][used][2]') * 1024 * 1024)"
                        remove_field => [ "[gc][heap][used_m][2]" ]
                    }
                }
                if [gc][heap][used_m][2] == "G" {
                    ruby {
                        code => "event.set('[gc][heap][used][2]', event.get('[gc][heap][used][2]') * 1024 * 1024 * 1024)"
                        remove_field => [ "[gc][heap][used_m][2]" ]
                    }
                }
                
                if [gc][heap][comm_m][1] == "B" {
                    mutate {
                        remove_field => [ "[gc][heap][comm_m][1]" ]
                    }
                }
                if [gc][heap][comm_m][1] == "K" {
                    ruby {
                        code => "event.set('[gc][heap][comm][1]', event.get('[gc][heap][comm][1]') * 1024)"
                        remove_field => [ "[gc][heap][comm_m][1]" ]
                    }
                }
                if [gc][heap][comm_m][1] == "M" {
                    ruby {
                        code => "event.set('[gc][heap][comm][1]', event.get('[gc][heap][comm][1]') * 1024 * 1024)"
                        remove_field => [ "[gc][heap][comm_m][1]" ]
                    }
                }
                if [gc][heap][comm_m][1] == "G" {
                    ruby {
                        code => "event.set('[gc][heap][comm][1]', event.get('[gc][heap][comm][1]') * 1024 * 1024 * 1024)"
                        remove_field => [ "[gc][heap][comm_m][1]" ]
                    }
                }
                
                if [gc][heap][comm_m][2] == "B" {
                    mutate {
                        remove_field => [ "[gc][heap][comm_m][2]" ]
                    }
                }
                if [gc][heap][comm_m][2] == "K" {
                    ruby {
                        code => "event.set('[gc][heap][comm][2]', event.get('[gc][heap][comm][2]') * 1024)"
                        remove_field => [ "[gc][heap][comm_m][2]" ]
                    }
                }
                if [gc][heap][comm_m][2] == "M" {
                    ruby {
                        code => "event.set('[gc][heap][comm][2]', event.get('[gc][heap][comm][2]') * 1024 * 1024)"
                        remove_field => [ "[gc][heap][comm_m][2]" ]
                    }
                }
                if [gc][heap][comm_m][2] == "G" {
                    ruby {
                        code => "event.set('[gc][heap][comm][2]', event.get('[gc][heap][comm][2]') * 1024 * 1024 * 1024)"
                        remove_field => [ "[gc][heap][comm_m][2]" ]
                    }
                }
                
                # Metaspace metrics
                
                if [gc][meta][used_m][1] == "B" {
                    mutate {
                        remove_field => [ "[gc][meta][used_m][1]" ]
                    }
                }
                if [gc][meta][used_m][1] == "K" {
                    ruby {
                        code => "event.set('[gc][meta][used][1]', event.get('[gc][meta][used][1]') * 1024)"
                        remove_field => [ "[gc][meta][used_m][1]" ]
                    }
                }
                if [gc][meta][used_m][1] == "M" {
                    ruby {
                        code => "event.set('[gc][meta][used][1]', event.get('[gc][meta][used][1]') * 1024 * 1024)"
                        remove_field => [ "[gc][meta][used_m][1]" ]
                    }
                }
                if [gc][meta][used_m][1] == "G" {
                    ruby {
                        code => "event.set('[gc][meta][used][1]', event.get('[gc][meta][used][1]') * 1024 * 1024 * 1024)"
                        remove_field => [ "[gc][meta][used_m][1]" ]
                    }
                }
                
                if [gc][meta][used_m][2] == "B" {
                    mutate {
                        remove_field => [ "[gc][meta][used_m][2]" ]
                    }
                }
                if [gc][meta][used_m][2] == "K" {
                    ruby {
                        code => "event.set('[gc][meta][used][2]', event.get('[gc][meta][used][2]') * 1024)"
                        remove_field => [ "[gc][meta][used_m][2]" ]
                    }
                }
                if [gc][meta][used_m][2] == "M" {
                    ruby {
                        code => "event.set('[gc][meta][used][2]', event.get('[gc][meta][used][2]') * 1024 * 1024)"
                        remove_field => [ "[gc][meta][used_m][2]" ]
                    }
                }
                if [gc][meta][used_m][2] == "G" {
                    ruby {
                        code => "event.set('[gc][meta][used][2]', event.get('[gc][meta][used][2]') * 1024 * 1024 * 1024)"
                        remove_field => [ "[gc][meta][used_m][2]" ]
                    }
                }
                
                if [gc][meta][comm_m] == "B" {
                    mutate {
                        remove_field => [ "[gc][meta][comm_m]" ]
                    }
                }
                if [gc][meta][comm_m] == "K" {
                    ruby {
                        code => "event.set('[gc][meta][comm]', event.get('[gc][meta][comm]') * 1024)"
                        remove_field => [ "[gc][meta][comm_m]" ]
                    }
                }
                if [gc][meta][comm_m] == "M" {
                    ruby {
                        code => "event.set('[gc][meta][comm]', event.get('[gc][meta][comm]') * 1024 * 1024)"
                        remove_field => [ "[gc][meta][comm_m]" ]
                    }
                }
                if [gc][meta][comm_m] == "G" {
                    ruby {
                        code => "event.set('[gc][meta][comm]', event.get('[gc][meta][comm]') * 1024 * 1024 * 1024)"
                        remove_field => [ "[gc][meta][comm_m]" ]
                    }
                }
                
            }
        }
    }

    # Alternatively extract grid host name from log source path
    if ![grid][host] {
        grok {
            patterns_dir => ["/etc/logstash/patterns"]
            patterns_files_glob => "env-logs-patterns"
            match => { "source" => "%{GRID_HOST:[grid][host]}" }
            remove_tag => [ "grid_host_failure" ]
            tag_on_failure => [ "grid_host_failure" ]
        }
    }
        
    # Alternatively extract grid host IP from log source path
    if ![grid][host] {
        grok {
            patterns_dir => ["/etc/logstash/patterns"]
            patterns_files_glob => "env-logs-patterns"
            match => { "source" => "%{IP:[grid][host]}" }
            remove_tag => [ "grid_host_failure" ]
            tag_on_failure => [ "grid_host_failure" ]
        }
    }
    
    # Set label for the grid host
    if [grid][host] {
        if [grid][client] == "false" {
            mutate {
                add_field => { "[grid][label]" => "%{[grid][host]} server" }
            }
        }
        else if [grid][client] == "true" {
            mutate {
                add_field => { "[grid][label]" => "%{[grid][host]} client" }
            }
        }
        else {
            mutate {
                add_field => { "[grid][label]" => "%{[grid][host]}" }
            }
        }
    }
    
    # Extract source file from log source path
    grok {
        patterns_dir => ["/etc/logstash/patterns"]
        patterns_files_glob => "env-logs-patterns"
        match => { "source" => "^.*[\\/]%{DATA:[src][file]}$" }
        tag_on_failure => [ "src_file_failure" ]
    }

    #if [level] in [ "ERROR", "SEVERE" ] {
    #    aggregate {
    #        task_id => "email_%{[grid][group]}"
    #        timeout => 60
    #        push_map_as_event_on_timeout => true
    #        code => "map['grid.group'] = event.get('[grid][group]'); map['message'] ||= ''; map['message'] += event.timestamp.to_s + ' [' + event.get('level').to_s + '] ' + event.get('message') + 10.chr + 10.chr"
    #        timeout_tags => [ "output_email" ]
    #    }
    #}
}

output {
    if "output_email" in [tags] {
        #email {
        #    to => 'user@domain'
        #    from => 'logstash@domain'
        #    subject => 'Logstash alert from %{grid.group}'
        #    body => "%{message}"
        #    domain => '127.0.0.1'
        #    port => 25
        #}
    } else {
        elasticsearch {
            hosts => [ "{{ hostvars[groups['elk'][0]]['inventory_hostname'] }}:{{ hostvars[groups['elk'][0]]['elastic_port'] }}" ]
            template => "/etc/logstash/templates/env-logs-index.json"
            template_name => "logstash-*"
            template_overwrite => true
        }
    }
    #stdout { codec => rubydebug }
}
