# The # character at the beginning of a line indicates a comment. Use
# comments to describe your configuration.

input {
    beats {
        port => "{{ logstash_port }}"
    }
}

filter {

    # Dstat header message pattern
    if ![message_kind] {
        grok {
            patterns_dir => ["/etc/logstash/patterns"]
            patterns_files_glob => "env-logs-patterns"
            match => { "message" => "(?<message>(?m)^\"Dstat \d+(\.\d+)* CSV output\".*)\"Date:\",\"%{TIMESTAMP:ts} \w+\"\n\n%{DATA:[csv][header][1]}\n%{DATA:[csv][header][2]}$" }
            overwrite => [ "message" ]
            add_field => { "message_kind" => "dstat" }
            remove_tag => [ "message_failure" ]
            tag_on_failure => [ "message_failure" ]
        }
    }
    if [message_kind] == "dstat" {
        ruby {
            init => "@@source_header = {}"
            code => "h1 = event.get('[csv][header][1]').gsub(' ','_').split(','); h2 = event.get('[csv][header][2]').gsub(' ','_').split(','); h2.each_index{ |i| if h1[i] == nil || h1[i] == '' then h1[i] = h1[i-1] end }; h2.each_index{ |i| h = 'dstat.' + h1[i][1..-2] + '.' + h2[i][1..-2]; h1[i] = h }; @@source_header[event.get('source')] = h1"
            remove_field => [ "[csv][header][1]", "[csv][header][2]" ]
        }
    }
    
    # GC log header message pattern
    if ![message_kind] {
        grok {
            patterns_dir => ["/etc/logstash/patterns"]
            patterns_files_glob => "env-logs-patterns"
            match => { "message" => "^Java.*\nMemory:\s*%{DATA:[gc][mem][page_size]} page, physical %{DATA:[gc][mem][phys][total]}\(%{DATA:[gc][mem][phys][free]} free\), swap %{DATA:[gc][mem][swap][total]}\(%{DATA:[gc][mem][swap][free]} free\)\nCommandLine flags:\s+%{DATA:[gc][flags]}\s*(\n|$)" }
            overwrite => [ "message" ]
            add_field => { "message_kind" => "gc" }
            remove_tag => [ "message_failure" ]
            tag_on_failure => [ "message_failure" ]
        }
    }

    # Thread dump header message pattern
    if ![message_kind] {
        grok {
            patterns_dir => ["/etc/logstash/patterns"]
            patterns_files_glob => "env-logs-patterns"
            match => { "message" => "^%{TIMESTAMP:ts}\n(?<message>Full thread dump Java.*)" }
            overwrite => [ "message" ]
            add_field => { "message_kind" => "td" }
            remove_tag => [ "message_failure" ]
            tag_on_failure => [ "message_failure" ]
        }
    }
    if [message_kind] == "td" {
        ruby {
            init => "@@source_ts = {}"
            code => "@@source_ts[event.get('source')] = event.get('ts')"
        }
    }

    if [message_kind] {
    
        # Remember source message kind
        ruby {
            init => "@@source_kind = {}"
            code => "@@source_kind[event.get('source')] = event.get('message_kind')"
        }
    } else {

        # Try to restore source message kind from cache
        ruby {
            code => "i = 0; mk = nil; while mk == nil && i < 5 do if i > 0 then sleep(0.01) end; mk = @@source_kind[event.get('source')]; i += 1 end; if mk != nil then event.set('message_kind', mk) end"
        }
    }
    
    # Default log full message pattern (splitting to timestamp, loglevel, thread, emitter and remaining message)
    if ![message_kind] {
        grok {
            patterns_dir => ["/etc/logstash/patterns"]
            patterns_files_glob => "env-logs-patterns"
            match => { "message" => "^\[?%{TIMESTAMP:ts}\]?\s*\[%{LOGLEVEL:level}\s*\]\s*(\[%{DATA:[thread][name]}\]\s*\[%{DATA:emitter}\]\s*)?(?<message>(.|\r|\n)*)" }
            match => { "message" => "^%{TIMESTAMP:ts}\s*\[%{LOGLEVEL:level}\s*\]\s*(\[%{DATA:emitter}\]\s*\[%{DATA:[thread][name]}\]\s*)?-\s*(?<message>(.|\r|\n)*)" }
            match => { "message" => "^%{TIMESTAMP:ts}\s*%{LOGLEVEL:level}\s*%{DATA:emitter}(:\d+)?\s*-\s*(?<message>(.|\r|\n)*)" }
            match => { "message" => "^%{TIMESTAMP:ts}\s*\[%{DATA:[thread][name]}\]\s*%{LOGLEVEL:level}\s+%{DATA:emitter}:\s*-\s*(?<message>(.|\r|\n)*)" }
            match => { "message" => "^%{TIME:tm}\s+\|-%{LOGLEVEL:level}\s+in\s+%{DATA:emitter}\s*-\s*(?<message>(.|\r|\n)*)" }
            match => { "message" => "^%{TIME:tm}\s*\[%{LOGLEVEL:level}\s*\]\s*\[%{DATA:emitter}\]\s*(\[T:\]\s*)?-\s*(?<message>(.|\r|\n)*)" }
            match => { "message" => "^%{TIME:tm}\s+%{LOGLEVEL:level}\s*\[%{DATA:[thread][name]}\]\s*%{DATA:emitter}\s*-\s*(?<message>(.|\r|\n)*)" }
            match => { "message" => "^\[\[%{DATA:[module][name]}\]-\[%{DATA:[module][service]}\]-%{DATA:[thread][name]}\]\s*%{TIME:tm}\s+%{LOGLEVEL:level}\s+%{DATA:emitter}:%{NONNEGINT:[line][number]:int} - (?<message>(.|\r|\n)*)" }
            match => { "message" => "^\[%{DATA:[thread][name]}\s*\|\s*%{DATA:[module][service]}\]\s*%{TIME:tm}\s+%{LOGLEVEL:level}\s+%{DATA:emitter}:%{NONNEGINT:[line][number]:int} - (?<message>(.|\r|\n)*)" }
            match => { "message" => "^(\[%{DATA:[thread][name]}\s*\]\s*)?%{TIME:tm}\s+%{LOGLEVEL:level}\s+%{DATA:emitter}:%{NONNEGINT:[line][number]:int} - (?<message>(.|\r|\n)*)" }
            match => { "message" => "^<%{TIME:tm}><%{DATA:[thread][name]}><%{DATA:emitter}>\s*(?<message>(.|\r|\n)*)" }
            overwrite => [ "message" ]
            add_field => { "message_kind" => "log" }
            remove_tag => [ "message_failure" ]
            tag_on_failure => [ "message_failure" ]
            timeout_millis => 0
        }
    }

    # Safepoint log full message pattern
    if ![message_kind] {
        grok {
            patterns_dir => ["/etc/logstash/patterns"]
            patterns_files_glob => "env-logs-patterns"
            match => { "message" => "^\s+vmop\s*\[threads: total initially_running wait_to_block\]\s*\[time: spin block sync cleanup vmop\] page_trap_count\n%{BASE10NUM:uptime:float}:\s*%{DATA:[safepoint][vmop]}\s*\[\s*%{NONNEGINT:[safepoint][threads][total]:int}\s+%{NONNEGINT:[safepoint][threads][initially_running]:int}\s+%{NONNEGINT:[safepoint][threads][wait_to_block]:int}\s*\]\s*\[\s*%{NONNEGINT:[safepoint][time][spin]:int}\s+%{NONNEGINT:[safepoint][time][block]:int}\s+%{NONNEGINT:[safepoint][time][sync]:int}\s+%{NONNEGINT:[safepoint][time][cleanup]:int}\s+%{NONNEGINT:[safepoint][time][vmop]:int}\s*\]\s*%{NONNEGINT:[safepoint][page_trap_count]:int}" }
            add_field => { "message_kind" => "safepoint" }
            remove_tag => [ "message_failure" ]
            tag_on_failure => [ "message_failure" ]
            timeout_millis => 0
        }
        if [message_kind] == "safepoint" {
            mutate {
                replace => { "message" => "vmop: %{[safepoint][vmop]}" }
            }
        }
    }

    # GC log full message pattern
    if ![message_kind] or ([message_kind] == "gc" and "message_failure" in [tags]) {
        grok {
            patterns_dir => ["/etc/logstash/patterns"]
            patterns_files_glob => "env-logs-patterns"
            match => { "message" => "^(%{TIMESTAMP:ts}:\s*)?%{BASE10NUM:uptime:float}:\s*(?<message>(.|\r|\n)*)" }
            overwrite => [ "message" ]
            remove_tag => [ "message_failure" ]
            tag_on_failure => [ "message_failure" ]
            timeout_millis => 0
        }
        if ![message_kind] {
            mutate {
                add_field => { "message_kind" => "gc" }
            }
        }
    }
    
    # Default log full message pattern (splitting to timestamp and remaining message)
    if ![message_kind] {
        grok {
            patterns_dir => ["/etc/logstash/patterns"]
            patterns_files_glob => "env-logs-patterns"
            match => { "message" => "^%{TIMESTAMP:ts}\s*(?<message>(.|\r|\n)*)" }
            match => { "message" => "^\[%{TIME:tm}\]\s*(?<message>(.|\r|\n)*)" }
            overwrite => [ "message" ]
            add_field => { "message_kind" => "log" }
            remove_tag => [ "message_failure" ]
                tag_on_failure => [ "message_failure" ]
                timeout_millis => 0
            }
    }

    # Thread dump message pattern
    if [message_kind] == "td" and "message_failure" in [tags] {
        grok {
            patterns_dir => ["/etc/logstash/patterns"]
            patterns_files_glob => "env-logs-patterns"
            match => { "message" => "^\"%{DATA:[thread][name]}\"\s+(#%{NONNEGINT:[thread][num]}\s+)?((?<thread.daemon>daemon)\s+)?(prio=%{NONNEGINT:[thread][prio]}\s+)?(os_prio=%{NONNEGINT:[thread][os_prio]}\s+)?tid=%{BASE16NUM:[thread][tid]}\s+nid=%{BASE16NUM:[thread][nid]}\s*%{DATA:[thread][status]}\s*(\[%{BASE16NUM:[thread][ptr]}\]\n\s*java.lang.Thread.State:\s*%{DATA:[thread][state]})?(\n|$)" }
            remove_tag => [ "message_failure" ]
            tag_on_failure => [ "message_failure" ]
        }
        ruby {
            code => "ts = @@source_ts[event.get('source')]; if ts != nil then event.set('ts',  ts) else event.tag('ts_nil') end"
        }
    }
    
    # Dstat message pattern
    if [message_kind] == "dstat" and "message_failure" in [tags] {
        ruby {
            code => "h = @@source_header[event.get('source')]; r = event.get('message').split(','); h.each_index{ |i| if h[i] == 'dstat.system.time' then event.set('ts', r[i]) else event.set(h[i], r[i].to_f) end }; event.set('message', '')"
            remove_tag => [ "message_failure" ]
        }
    }
    
    # Last date processing
    if [ts] {
    
        # Remember last date
        grok {
            patterns_dir => ["/etc/logstash/patterns"]
            patterns_files_glob => "env-logs-patterns"
            match => { "ts" => "^(?<dt>\d{4}[-/\.]\d{2}[-/\.]\d{2})" }
            tag_on_failure => [ "dt_failure" ]
        }
        if !("dt_failure" in [tags]) {
            ruby {
                init => "@@source_date = {}"
                code => "@@source_date[event.get('source')] = event.get('dt')"
                remove_field => [ "dt" ]
            }
        }
    }
    else if [tm] {
    
        # Constructing timestamp from time and last date 
        ruby {
            code => "i = 0; dt = nil; while dt == nil && i < 10 do if i > 0 then sleep(0.01) end; dt = @@source_date[event.get('source')]; i += 1 end; if dt != nil then event.set('ts',  dt + ' ' + event.get('tm')) else event.tag('dt_nil') end"
            remove_field => [ "tm" ]
        }
    }
    
    # Uptime processing
    if [ts] {

        # Parsing log timestamp
        date {
            locale => "en"
            match => ["ts", "YYYY-MM-dd'T'HH:mm:ss.SSSZ", "YYYY-MM-dd HH:mm:ss.SSS", "YYYY-MM-dd HH:mm:ss,SSS", "YYYY.MM.dd HH:mm:ss.SSS", "YYYY-MM-dd HH:mm:ss", "YYYY.MM.dd HH:mm:ss", "dd MMM YYYY HH:mm:ss", "dd-MM HH:mm:ss", "HH:mm:ss" ]
            timezone => "Europe/Moscow"
            target => "@timestamp"
            remove_field => [ "ts" ]
            tag_on_failure => [ "ts_failure" ]
        }
    } else if [uptime] {
    
        # Calculating log timestamp from source start timestamp
        ruby {
            init => "@@source_start = {}"
            code => "ss = @@source_start[event.get('source')]; if ss != nil then event.set('ss', ss) end"
        }
        if ![ss] {
            grok {
                patterns_dir => ["patterns"]
                patterns_files_glob => "env-logs-patterns"
                match => { "source" => "-(?<dt>\d{8}-\d{6})\b" }
                tag_on_failure => [ "ss_dt_failure" ]
            }
            if [dt] {
                date {
                    locale => "en"
                    match => ["dt", "YYYYMMdd'-'HHmmss" ]
                    timezone => "Europe/Moscow"
                    target => "ss"
                    remove_field => [ "dt" ]
                    tag_on_failure => [ "ss_ts_failure" ]
                }
            } else {
                ruby {
                    code => "event.set('ss', LogStash::Timestamp.now)"
                }
            }
            ruby {
                code => "@@source_start[event.get('source')] = event.get('ss')"
            }
        }
        if [ss] {
            ruby {
                code => "event.timestamp = LogStash::Timestamp.at(event.get('ss').time.to_f + event.get('uptime'))"
            }
        }
    }

    # Fix log level
    grok {
        patterns_dir => ["/etc/logstash/patterns"]
        patterns_files_glob => "env-logs-patterns"
        match => { "[message]" => "^Logging at %{LOGLEVEL:level} level without checking if %{LOGLEVEL:[@metadata][level]} level is enabled: (?m)%{GREEDYDATA:message}" }
        overwrite => [ "level", "message" ]
        tag_on_failure => []
    }
    
    # Default log level INFO
    if ![level] {
        mutate {
            add_field => { "level" => "INFO" }
        }
    }
   
    # Parsing thread name, grid name and grid host
    if [message_kind] == "log" or [message_kind] == "td" {

        # Parsing thread name to prefix, number and grid name
        grok {
            patterns_dir => ["/etc/logstash/patterns"]
            patterns_files_glob => "env-logs-patterns"
            match => { "[thread][name]" => "^%{DATA:[thread][prefix]}-#%{INT:[thread][number]:int}\%%{DATA:[grid][name]}\%$" }
            tag_on_failure => [ "thread_failure" ]
        }
    
        # Try to parse grid name from message
        if ![grid][name] {
            grok {
                patterns_dir => ["/etc/logstash/patterns"]
                patterns_files_glob => "env-logs-patterns"
                match => { "[message]" => "Ignite.*?instance\s*name\s*[:=]\s*%{DATA:[grid][name]}([\s,\)\]]|$)" }
                tag_on_failure => [ "grid_name_failure" ]
            }
        }
        
        if ![grid][host] {
            # Extracting grid host from grid name
            grok {
                patterns_dir => ["/etc/logstash/patterns"]
                patterns_files_glob => "env-logs-patterns"
                match => { "[grid][name]" => "^%{DATA:[grid][name]}\%(?<grid.host>grid\d+)$" }
                remove_tag => [ "grid_host_failure" ]
                tag_on_failure => [ "grid_host_failure" ]
            }
        }

        if ![grid][host] {
            # Try to parse grid host from message
            grok {
                patterns_dir => ["/etc/logstash/patterns"]
                patterns_files_glob => "env-logs-patterns"
                match => { "message" => "host\s+name:\s+%{WORD:[grid][host]}" }
                remove_tag => [ "grid_host_failure" ]
                tag_on_failure => [ "grid_host_failure" ]
            }
        }
        
        if [grid][host] {
        
            # Remember grid host
            ruby {
                init => "@@source_grid_host = {}"
                code => "@@source_grid_host[event.get('source')] = event.get('[grid][host]')"
            }
        } else {
 
            # Try to restore grid host from cache
            ruby {
                code => "i = 0; host = nil; while host == nil && i < 10 do if i > 0 then sleep(0.01) end; host = @@source_grid_host[event.get('source')]; i += 1 end; if host != nil then event.set('[grid][host]', host) else event.tag('source_grid_host_failure') end"
                remove_tag => [ "grid_host_failure" ]
            }
        }
    }
        
	# Parsing default log specific messages
    if [message_kind] == "log" {

        if ![message_type] {
            grok {
                patterns_dir => ["/etc/logstash/patterns"]
                patterns_files_glob => "env-logs-patterns"
                match => { "message" => "^Topology snapshot \[ver=%{NONNEGINT:[top][ver]:int}, servers=%{NONNEGINT:[top][servers]:int}, clients=%{NONNEGINT:[top][clients]:int}, CPUs=%{NONNEGINT:[top][cpu_count]:int}, heap=%{BASE10NUM:[top][heap_size_gb]:float}GB\]" }
                add_field => { "message_type" => "top_snap" }
                tag_on_failure => []
            }
        }
        if ![message_type] {
            grok {
                patterns_dir => ["/etc/logstash/patterns"]
                patterns_files_glob => "env-logs-patterns"
                match => { "message" => "^Metrics for local node \(to disable set \'metricsLogFrequency\' to 0\)\n\s+\^-- Node \[id=%{DATA:[node][short_id]}, %{DATA:[node][name]}, uptime=%{DATA:[node][uptime]}\]\n\s+\^-- H/N/C \[hosts=%{NONNEGINT:[top][hosts]:int}, nodes=%{NONNEGINT:[top][nodes]:int}, CPUs=%{NONNEGINT:[top][cpu_count]:int}\]\n\s+\^-- CPU \[cur=%{BASE10NUM:[cpu][current]:float}\%, avg=%{BASE10NUM:[cpu][avg]:float}\%, GC=%{BASE10NUM:[cpu][gc]:float}\%\]\n\s+\^-- PageMemory \[pages=%{NONNEGINT:[mem][pages]:int}\]\n\s+\^-- Heap \[used=%{NONNEGINT:[heap][used_mb]:int}MB, free=%{BASE10NUM:[heap][free]:float}\%, comm=%{NONNEGINT:[heap][comm_mb]:int}MB\]\n\s+\^-- Non heap \[used=%{NONNEGINT:[non_heap][used_mb]:int}MB, free=%{BASE10NUM:[non_heap][free]:float}\%, comm=%{NONNEGINT:[non_heap]:int}MB\]\n\s+\^-- Public thread pool \[active=%{NONNEGINT:[thread_pool][pub][active]:int}, idle=%{NONNEGINT:[thread_pool][pub][idle]:int}, qSize=%{NONNEGINT:[thread_pool][pub][queue_size]:int}\]\n\s+\^-- System thread pool \[active=%{NONNEGINT:[thread_pool][sys][active]:int}, idle=%{NONNEGINT:[thread_pool][sys][idle]:int}, qSize=%{NONNEGINT:[thread_pool][sys][queue_size]:int}\]\n\s+\^-- Outbound messages queue \[size=%{NONNEGINT:[outbound_msg_queue][size]:int}\]" }
                add_field => { "message_type" => "metrics" }
                tag_on_failure => []
            }
        }
        if ![message_type] {
            grok {
                patterns_dir => ["/etc/logstash/patterns"]
                patterns_files_glob => "env-logs-patterns"
                match => { "message" => "^Checkpoint started \[checkpointId=%{UUID:[cp][id]}, startPtr=FileWALPointer \[idx=%{NONNEGINT:[wal][idx]:int}, fileOffset=%{NONNEGINT:[wal][off]:int}, len=%{NONNEGINT:[wal][len]:int}(, forceFlush=%{WORD:[wal][force_flush]})?\], checkpointLockWait=%{NONNEGINT:[cp][lock_wait_time]:int}ms, checkpointLockHoldTime=%{NONNEGINT:[cp][lock_hold_time]:int}ms, pages=%{NONNEGINT:[cp][pages]:int}, reason='%{WORD:[cp][reason]}'\]" }
                add_field => { "message_type" => "cp_start" }
                tag_on_failure => []
            }
        }
        if ![message_type] {
            grok {
                patterns_dir => ["/etc/logstash/patterns"]
                patterns_files_glob => "env-logs-patterns"
                match => { "message" => "^Checkpoint finished \[cpId=%{UUID:[cp][id]}, pages=%{NONNEGINT:[cp][pages]:int}, markPos=FileWALPointer \[idx=%{NONNEGINT:[wal][idx]:int}, fileOffset=%{NONNEGINT:[wal][off]:int}, len=%{NONNEGINT:[wal][len]:int}(, forceFlush=%{WORD:[wal][force_flush]})?\], walSegmentsCleared=%{NONNEGINT:[wal][segs_cleared]:int}(, walHistorySize=%{NONNEGINT:[wal][hist_size]:int})?(, markBegin=%{NONNEGINT:[cp][mark_begin_time]:int}ms)?(, markDuration=%{NONNEGINT:[cp][mark_begin_time]:int}ms)?, pagesWrite=%{NONNEGINT:[cp][pages_write_time]:int}ms, fsync=%{NONNEGINT:[cp][fsync_time]:int}ms(, markEnd=%{NONNEGINT:[cp][mark_end_time]:int}ms)?, total=%{NONNEGINT:[cp][total_time]:int}ms\]" }
                add_field => { "message_type" => "cp_finish" }
                tag_on_failure => []
            }
            if [message_type] == "cp_finish" and [cp][total_time] > 0 {
                ruby {
                    code => "event.set('[cp][throughput]', event.get('[cp][pages]') * 1000 / event.get('[cp][total_time]'))"
                }
            }
        }

        if ![message_type] {
            grok {
                patterns_dir => ["/etc/logstash/patterns"]
                patterns_files_glob => "env-logs-patterns"
                match => { "message" => "^Failed to wait for partition map exchange \[topVer=AffinityTopologyVersion \[topVer=%{NONNEGINT:[pme][top][ver]:int}, minorTopVer=%{NONNEGINT:[pme][top][minor]:int}\], node=%{UUID:[pme][node][id]}\]" }
                add_field => { "message_type" => "pe_wait" }
                tag_on_failure => []
            }
        }

        if ![message_type] {
            grok {
                patterns_dir => ["/etc/logstash/patterns"]
                patterns_files_glob => "env-logs-patterns"
                match => { "message" => "^Failed to wait for partition release future \[topVer=AffinityTopologyVersion \[topVer=%{NONNEGINT:[prf][top][ver]:int}, minorTopVer=%{NONNEGINT:[prf][top][minor]:int}\], node=%{UUID:[prf][node][id]}\]" }
                add_field => { "message_type" => "pe_wait" }
                tag_on_failure => []
            }
        }

        if ![message_type] {
            grok {
                patterns_dir => ["/etc/logstash/patterns"]
                patterns_files_glob => "env-logs-patterns"
                match => { "message" => "^Ready affinity version: AffinityTopologyVersion \[topVer=%{NONNEGINT:[top][ver]:int}, minorTopVer=%{NONNEGINT:[top][minor]:int}\]" }
                add_field => { "message_type" => "pe_debug" }
                tag_on_failure => []
            }
        }

        if ![message_type] {
            grok {
                patterns_dir => ["/etc/logstash/patterns"]
                patterns_files_glob => "env-logs-patterns"
                match => { "message" => "^Last exchange future: " }
                add_field => { "message_type" => "pe_debug" }
                tag_on_failure => []
            }
        }

        if ![message_type] {
            grok {
                patterns_dir => ["/etc/logstash/patterns"]
                patterns_files_glob => "env-logs-patterns"
                match => { "message" => "^Last %{NONNEGINT:[pme][futures][shown]} exchange futures \(total: %{NONNEGINT:[pme][futures][total]}\):" }
                add_field => { "message_type" => "pe_debug" }
                tag_on_failure => []
            }
        }

        if ![message_type] {
            grok {
                patterns_dir => ["/etc/logstash/patterns"]
                patterns_files_glob => "env-logs-patterns"
                match => { "message" => "^(Pending transactions:|Pending explicit locks:|Pending cache futures:|Pending atomic cache futures:|Pending data streamer futures:|Pending transaction deadlock detection futures:)" }
                add_field => { "message_type" => "pe_debug" }
                tag_on_failure => []
            }
        }

        if ![message_type] {
            grok {
                patterns_dir => ["/etc/logstash/patterns"]
                patterns_files_glob => "env-logs-patterns"
                match => { "message" => "^>>>\s+\w+\s+\[" }
                add_field => { "message_type" => "pe_debug" }
                tag_on_failure => []
            }
        }

        if [message_type] in [ "pe_wait", "pe_debug"] {

            ruby {
                init => "require 'securerandom'"
                code => "event.set('level', 'DEBUG')"
            }

            aggregate {
                task_id => "%{source}"
                inactivity_timeout => 600
                code => "id = nil; ts = event.timestamp.time.to_f; map.each { |k, v| if (ts > (v - 5.0)) and (ts < (v + 5.0)) then id = k; break; end; }; if id == nil then id = SecureRandom.uuid; map[id] = ts; end; event.set('aggregate_id', id)"
            }
        }

        if [message_type] == "pe_wait" {

            aggregate {
                task_id => "%{aggregate_id}"
                inactivity_timeout => 10
                push_map_as_event_on_timeout => true
                code => "map.merge!(event.to_hash)"
                timeout_code => "event.set('level', 'WARN'); event.set('message_type', 'pe_aggregate'); m = event.get('message'); dm = {}; event.to_hash.each { |k, v| if k.start_with?('debug_message_') then off = k['debug_message_'.length..-1].to_i; dm[off] = v; end }; dm.sort.map { |off, msg| m += 10.chr + 10.chr + msg; event.remove('debug_message_' + off.to_s) }; event.set('message', m)"
            }
        }

        if [message_type] == "pe_debug" {

            aggregate {
                task_id => "%{aggregate_id}"
                code => "m = event.get('message'); if m.length > 10000 then m = (m[0..4999] + '... ' + (m.length - 10000).to_s + ' char(s) ommited ...' + m[-5000..-1]) end; map['debug_message_' + event.get('offset').to_s] = m"
            }
        }

        # Fix log level to CONFIG
        grok {
            patterns_dir => ["/etc/logstash/patterns"]
            patterns_files_glob => "env-logs-patterns"
            match => { "message" => "^Checkpoints are disabled" }
            match => { "message" => "^Collision resolution is disabled" }
            match => { "message" => "^Failure detection timeout will be ignored" }
            match => { "message" => "^Message queue limit is set to 0" }
            match => { "message" => "^Peer class loading is enabled" }
            match => { "message" => "^Rolling updates are disabled" }
            match => { "message" => "^User or internal attribute has the same name as environment or system property and will take precedence" }
            remove_field => [ "level" ]
            tag_on_failure => []
        }
        if ![level] {
            mutate {
                add_field => { "level" => "CONFIG" }
            }
        }
   
    }
    
    # Parsing GC log specific messages
    if [message_kind] == "gc" {
    
        if ![message_type] {
            grok {
                patterns_dir => ["/etc/logstash/patterns"]
                patterns_files_glob => "env-logs-patterns"
                match => { "message" => "^Total time for which application threads were stopped: %{BASE10NUM:[app][pause][total_time]:float} seconds, Stopping threads took: %{BASE10NUM:[app][pause][threads_time]:float} seconds" }
                add_field => { "message_type" => "app_pause" }
                tag_on_failure => []
            }
		}
        if ![message_type] {
            grok {
                patterns_dir => ["/etc/logstash/patterns"]
                patterns_files_glob => "env-logs-patterns"
                match => { "message" => "^\[%{DATA:[gc][msg][type]}\s*([,\]\d]|\(%{DATA:[gc][pause][type]}\))" }
                add_field => { "message_type" => "gc_pause" }
                tag_on_failure => []
            }
            
            if [message_type] == "gc_pause" {

				grok {
					patterns_dir => ["/etc/logstash/patterns"]
					patterns_files_glob => "env-logs-patterns"
					match => { "message" => "\[Eden: %{BASE10NUM:[gc][eden][used][1]:int}%{SIZE_M:[gc][eden][used_m][1]}\(%{BASE10NUM:[gc][eden][comm][1]:int}%{SIZE_M:[gc][eden][comm_m][1]}\)->%{BASE10NUM:[gc][eden][used][2]:int}%{SIZE_M:[gc][eden][used_m][2]}\(%{BASE10NUM:[gc][eden][comm][2]:int}%{SIZE_M:[gc][eden][comm_m][2]}\) Survivors: %{BASE10NUM:[gc][surv][1]:int}%{SIZE_M:[gc][surv_m][1]}->%{BASE10NUM:[gc][surv][2]:int}%{SIZE_M:[gc][surv_m][2]} Heap: %{BASE10NUM:[gc][heap][used][1]:int}%{SIZE_M:[gc][heap][used_m][1]}\(%{BASE10NUM:[gc][heap][comm][1]:int}%{SIZE_M:[gc][heap][comm_m][1]}\)->%{BASE10NUM:[gc][heap][used][2]:int}%{SIZE_M:[gc][heap][used_m][2]}\(%{BASE10NUM:[gc][heap][comm][2]:int}%{SIZE_M:[gc][heap][comm_m][2]}\)\]" }
					tag_on_failure => []
				}

				grok {
					patterns_dir => ["/etc/logstash/patterns"]
					patterns_files_glob => "env-logs-patterns"
					match => { "message" => "\[Metaspace: %{BASE10NUM:[gc][meta][used][1]:int}%{SIZE_M:[gc][meta][used_m][1]}->%{BASE10NUM:[gc][meta][used][2]:int}%{SIZE_M:[gc][meta][used_m][2]}\(%{BASE10NUM:[gc][meta][comm]:int}%{SIZE_M:[gc][meta][comm_m]}\)\]" }
					tag_on_failure => []
				}

				grok {
					patterns_dir => ["/etc/logstash/patterns"]
					patterns_files_glob => "env-logs-patterns"
					match => { "message" => "\[Times: user=%{BASE10NUM:[gc][pause][time][user]:float} sys=%{BASE10NUM:[gc][pause][time][sys]:float}, real=%{BASE10NUM:[gc][pause][time][real]:float} secs\]" }
					add_field => { "[app][pause][total_time]" => "%{[gc][pause][time][real]}" }
					tag_on_failure => []
				}

				grok {
					patterns_dir => ["/etc/logstash/patterns"]
					patterns_files_glob => "env-logs-patterns"
					match => { "message" => "^\s*%{BASE10NUM:[gc][heap][used][1]:int}%{SIZE_M:[gc][heap][used_m][1]}->%{BASE10NUM:[gc][heap][used][2]:int}%{SIZE_M:[gc][heap][used_m][2]}\(%{BASE10NUM:[gc][heap][comm][2]:int}%{SIZE_M:[gc][heap][comm_m][2]}\), %{BASE10NUM:[gc][pause][time][real]:float} secs\]" }
					add_field => { "[app][pause][total_time]" => "%{[gc][pause][time][real]}" }
					tag_on_failure => []
				}
            
                # Eden metrics
            
                if [gc][eden][used_m][1] == "B" {
                    mutate {
                        remove_field => [ "[gc][eden][used_m][1]" ]
                    }
                }
                if [gc][eden][used_m][1] == "K" {
                    ruby {
                        code => "event.set('[gc][eden][used][1]', event.get('[gc][eden][used][1]') * 1024)"
                        remove_field => [ "[gc][eden][used_m][1]" ]
                    }
                }
                if [gc][eden][used_m][1] == "M" {
                    ruby {
                        code => "event.set('[gc][eden][used][1]', event.get('[gc][eden][used][1]') * 1024 * 1024)"
                        remove_field => [ "[gc][eden][used_m][1]" ]
                    }
                }
                if [gc][eden][used_m][1] == "G" {
                    ruby {
                        code => "event.set('[gc][eden][used][1]', event.get('[gc][eden][used][1]') * 1024 * 1024 * 1024)"
                        remove_field => [ "[gc][eden][used_m][1]" ]
                    }
                }
                
                if [gc][eden][used_m][2] == "B" {
                    mutate {
                        remove_field => [ "[gc][eden][used_m][2]" ]
                    }
                }
                if [gc][eden][used_m][2] == "K" {
                    ruby {
                        code => "event.set('[gc][eden][used][2]', event.get('[gc][eden][used][2]') * 1024)"
                        remove_field => [ "[gc][eden][used_m][2]" ]
                    }
                }
                if [gc][eden][used_m][2] == "M" {
                    ruby {
                        code => "event.set('[gc][eden][used][2]', event.get('[gc][eden][used][2]') * 1024 * 1024)"
                        remove_field => [ "[gc][eden][used_m][2]" ]
                    }
                }
                if [gc][eden][used_m][2] == "G" {
                    ruby {
                        code => "event.set('[gc][eden][used][2]', event.get('[gc][eden][used][2]') * 1024 * 1024 * 1024)"
                        remove_field => [ "[gc][eden][used_m][2]" ]
                    }
                }
                
                if [gc][eden][comm_m][1] == "B" {
                    mutate {
                        remove_field => [ "[gc][eden][comm_m][1]" ]
                    }
                }
                if [gc][eden][comm_m][1] == "K" {
                    ruby {
                        code => "event.set('[gc][eden][comm][1]', event.get('[gc][eden][comm][1]') * 1024)"
                        remove_field => [ "[gc][eden][comm_m][1]" ]
                    }
                }
                if [gc][eden][comm_m][1] == "M" {
                    ruby {
                        code => "event.set('[gc][eden][comm][1]', event.get('[gc][eden][comm][1]') * 1024 * 1024)"
                        remove_field => [ "[gc][eden][comm_m][1]" ]
                    }
                }
                if [gc][eden][comm_m][1] == "G" {
                    ruby {
                        code => "event.set('[gc][eden][comm][1]', event.get('[gc][eden][comm][1]') * 1024 * 1024 * 1024)"
                        remove_field => [ "[gc][eden][comm_m][1]" ]
                    }
                }
                
                if [gc][eden][comm_m][2] == "B" {
                    mutate {
                        remove_field => [ "[gc][eden][comm_m][2]" ]
                    }
                }
                if [gc][eden][comm_m][2] == "K" {
                    ruby {
                        code => "event.set('[gc][eden][comm][2]', event.get('[gc][eden][comm][2]') * 1024)"
                        remove_field => [ "[gc][eden][comm_m][2]" ]
                    }
                }
                if [gc][eden][comm_m][2] == "M" {
                    ruby {
                        code => "event.set('[gc][eden][comm][2]', event.get('[gc][eden][comm][2]') * 1024 * 1024)"
                        remove_field => [ "[gc][eden][comm_m][2]" ]
                    }
                }
                if [gc][eden][comm_m][2] == "G" {
                    ruby {
                        code => "event.set('[gc][eden][comm][2]', event.get('[gc][eden][comm][2]') * 1024 * 1024 * 1024)"
                        remove_field => [ "[gc][eden][comm_m][2]" ]
                    }
                }
                
                # Survivor metrics
                
                if [gc][surv_m][1] == "B" {
                    mutate {
                        remove_field => [ "[gc][surv_m][1]" ]
                    }
                }
                if [gc][surv_m][1] == "K" {
                    ruby {
                        code => "event.set('[gc][surv][1]', event.get('[gc][surv][1]') * 1024)"
                        remove_field => [ "[gc][surv_m][1]" ]
                    }
                }
                if [gc][surv_m][1] == "M" {
                    ruby {
                        code => "event.set('[gc][surv][1]', event.get('[gc][surv][1]') * 1024 * 1024)"
                        remove_field => [ "[gc][surv_m][1]" ]
                    }
                }
                if [gc][surv_m][1] == "G" {
                    ruby {
                        code => "event.set('[gc][surv][1]', event.get('[gc][surv][1]') * 1024 * 1024 * 1024)"
                        remove_field => [ "[gc][surv_m][1]" ]
                    }
                }
                
                if [gc][surv_m][2] == "B" {
                    mutate {
                        remove_field => [ "[gc][surv_m][2]" ]
                    }
                }
                if [gc][surv_m][2] == "K" {
                    ruby {
                        code => "event.set('[gc][surv][2]', event.get('[gc][surv][2]') * 1024)"
                        remove_field => [ "[gc][surv_m][2]" ]
                    }
                }
                if [gc][surv_m][2] == "M" {
                    ruby {
                        code => "event.set('[gc][surv][2]', event.get('[gc][surv][2]') * 1024 * 1024)"
                        remove_field => [ "[gc][surv_m][2]" ]
                    }
                }
                if [gc][surv_m][2] == "G" {
                    ruby {
                        code => "event.set('[gc][surv][2]', event.get('[gc][surv][2]') * 1024 * 1024 * 1024)"
                        remove_field => [ "[gc][surv_m][2]" ]
                    }
                }
                
                # Heap metrics
                
                if [gc][heap][used_m][1] == "B" {
                    mutate {
                        remove_field => [ "[gc][heap][used_m][1]" ]
                    }
                }
                if [gc][heap][used_m][1] == "K" {
                    ruby {
                        code => "event.set('[gc][heap][used][1]', event.get('[gc][heap][used][1]') * 1024)"
                        remove_field => [ "[gc][heap][used_m][1]" ]
                    }
                }
                if [gc][heap][used_m][1] == "M" {
                    ruby {
                        code => "event.set('[gc][heap][used][1]', event.get('[gc][heap][used][1]') * 1024 * 1024)"
                        remove_field => [ "[gc][heap][used_m][1]" ]
                    }
                }
                if [gc][heap][used_m][1] == "G" {
                    ruby {
                        code => "event.set('[gc][heap][used][1]', event.get('[gc][heap][used][1]') * 1024 * 1024 * 1024)"
                        remove_field => [ "[gc][heap][used_m][1]" ]
                    }
                }
                
                if [gc][heap][used_m][2] == "B" {
                    mutate {
                        remove_field => [ "[gc][heap][used_m][2]" ]
                    }
                }
                if [gc][heap][used_m][2] == "K" {
                    ruby {
                        code => "event.set('[gc][heap][used][2]', event.get('[gc][heap][used][2]') * 1024)"
                        remove_field => [ "[gc][heap][used_m][2]" ]
                    }
                }
                if [gc][heap][used_m][2] == "M" {
                    ruby {
                        code => "event.set('[gc][heap][used][2]', event.get('[gc][heap][used][2]') * 1024 * 1024)"
                        remove_field => [ "[gc][heap][used_m][2]" ]
                    }
                }
                if [gc][heap][used_m][2] == "G" {
                    ruby {
                        code => "event.set('[gc][heap][used][2]', event.get('[gc][heap][used][2]') * 1024 * 1024 * 1024)"
                        remove_field => [ "[gc][heap][used_m][2]" ]
                    }
                }
                
                if [gc][heap][comm_m][1] == "B" {
                    mutate {
                        remove_field => [ "[gc][heap][comm_m][1]" ]
                    }
                }
                if [gc][heap][comm_m][1] == "K" {
                    ruby {
                        code => "event.set('[gc][heap][comm][1]', event.get('[gc][heap][comm][1]') * 1024)"
                        remove_field => [ "[gc][heap][comm_m][1]" ]
                    }
                }
                if [gc][heap][comm_m][1] == "M" {
                    ruby {
                        code => "event.set('[gc][heap][comm][1]', event.get('[gc][heap][comm][1]') * 1024 * 1024)"
                        remove_field => [ "[gc][heap][comm_m][1]" ]
                    }
                }
                if [gc][heap][comm_m][1] == "G" {
                    ruby {
                        code => "event.set('[gc][heap][comm][1]', event.get('[gc][heap][comm][1]') * 1024 * 1024 * 1024)"
                        remove_field => [ "[gc][heap][comm_m][1]" ]
                    }
                }
                
                if [gc][heap][comm_m][2] == "B" {
                    mutate {
                        remove_field => [ "[gc][heap][comm_m][2]" ]
                    }
                }
                if [gc][heap][comm_m][2] == "K" {
                    ruby {
                        code => "event.set('[gc][heap][comm][2]', event.get('[gc][heap][comm][2]') * 1024)"
                        remove_field => [ "[gc][heap][comm_m][2]" ]
                    }
                }
                if [gc][heap][comm_m][2] == "M" {
                    ruby {
                        code => "event.set('[gc][heap][comm][2]', event.get('[gc][heap][comm][2]') * 1024 * 1024)"
                        remove_field => [ "[gc][heap][comm_m][2]" ]
                    }
                }
                if [gc][heap][comm_m][2] == "G" {
                    ruby {
                        code => "event.set('[gc][heap][comm][2]', event.get('[gc][heap][comm][2]') * 1024 * 1024 * 1024)"
                        remove_field => [ "[gc][heap][comm_m][2]" ]
                    }
                }
                
                # Metaspace metrics
                
                if [gc][meta][used_m][1] == "B" {
                    mutate {
                        remove_field => [ "[gc][meta][used_m][1]" ]
                    }
                }
                if [gc][meta][used_m][1] == "K" {
                    ruby {
                        code => "event.set('[gc][meta][used][1]', event.get('[gc][meta][used][1]') * 1024)"
                        remove_field => [ "[gc][meta][used_m][1]" ]
                    }
                }
                if [gc][meta][used_m][1] == "M" {
                    ruby {
                        code => "event.set('[gc][meta][used][1]', event.get('[gc][meta][used][1]') * 1024 * 1024)"
                        remove_field => [ "[gc][meta][used_m][1]" ]
                    }
                }
                if [gc][meta][used_m][1] == "G" {
                    ruby {
                        code => "event.set('[gc][meta][used][1]', event.get('[gc][meta][used][1]') * 1024 * 1024 * 1024)"
                        remove_field => [ "[gc][meta][used_m][1]" ]
                    }
                }
                
                if [gc][meta][used_m][2] == "B" {
                    mutate {
                        remove_field => [ "[gc][meta][used_m][2]" ]
                    }
                }
                if [gc][meta][used_m][2] == "K" {
                    ruby {
                        code => "event.set('[gc][meta][used][2]', event.get('[gc][meta][used][2]') * 1024)"
                        remove_field => [ "[gc][meta][used_m][2]" ]
                    }
                }
                if [gc][meta][used_m][2] == "M" {
                    ruby {
                        code => "event.set('[gc][meta][used][2]', event.get('[gc][meta][used][2]') * 1024 * 1024)"
                        remove_field => [ "[gc][meta][used_m][2]" ]
                    }
                }
                if [gc][meta][used_m][2] == "G" {
                    ruby {
                        code => "event.set('[gc][meta][used][2]', event.get('[gc][meta][used][2]') * 1024 * 1024 * 1024)"
                        remove_field => [ "[gc][meta][used_m][2]" ]
                    }
                }
                
                if [gc][meta][comm_m] == "B" {
                    mutate {
                        remove_field => [ "[gc][meta][comm_m]" ]
                    }
                }
                if [gc][meta][comm_m] == "K" {
                    ruby {
                        code => "event.set('[gc][meta][comm]', event.get('[gc][meta][comm]') * 1024)"
                        remove_field => [ "[gc][meta][comm_m]" ]
                    }
                }
                if [gc][meta][comm_m] == "M" {
                    ruby {
                        code => "event.set('[gc][meta][comm]', event.get('[gc][meta][comm]') * 1024 * 1024)"
                        remove_field => [ "[gc][meta][comm_m]" ]
                    }
                }
                if [gc][meta][comm_m] == "G" {
                    ruby {
                        code => "event.set('[gc][meta][comm]', event.get('[gc][meta][comm]') * 1024 * 1024 * 1024)"
                        remove_field => [ "[gc][meta][comm_m]" ]
                    }
                }
                
            }
        }
    }

    # Alternatively extract grid host name from log source path
    if ![grid][host] {
        grok {
            patterns_dir => ["/etc/logstash/patterns"]
            patterns_files_glob => "env-logs-patterns"
            match => { "source" => "(?<grid.host>grid\d+)" }
            remove_tag => [ "grid_host_failure" ]
            tag_on_failure => [ "grid_host_failure" ]
        }
    }
        
    # Alternatively extract grid host IP from log source path
    if ![grid][host] {
        grok {
            patterns_dir => ["/etc/logstash/patterns"]
            patterns_files_glob => "env-logs-patterns"
            match => { "source" => "%{IP:[grid][host]}" }
            remove_tag => [ "grid_host_failure" ]
            tag_on_failure => [ "grid_host_failure" ]
        }
    }
    
    # Extract source file from log source path
    grok {
        patterns_dir => ["/etc/logstash/patterns"]
        patterns_files_glob => "env-logs-patterns"
        match => { "source" => "^.*[\\/]%{DATA:[src][file]}$" }
        tag_on_failure => [ "src_file_failure" ]
    }
}

output {
    elasticsearch {
        hosts => [ "{{ hostvars[groups['elk'][0]]['inventory_hostname'] }}:{{ hostvars[groups['elk'][0]]['elastic_port'] }}" ]
        template => "/etc/logstash/templates/env-logs-index.json"
        template_name => "logstash-*"
        template_overwrite => true
    }
    #stdout { codec => rubydebug }
}
